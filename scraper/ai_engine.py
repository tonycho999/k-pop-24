import sys
import os
import time
from datetime import datetime
from dotenv import load_dotenv

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS, TOP_RANK_LIMIT

load_dotenv()

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # [1ë‹¨ê³„] ì”¨ì•— ìˆ˜ì§‘
        seed_titles = []
        try:
            for seed in seeds:
                news = crawler.get_naver_api_news(seed, display=20)
                seed_titles.extend([n['title'] for n in news])
            seed_titles = list(set(seed_titles))
            print(f"   ğŸŒ± ì›ì„ ìˆ˜ì§‘ ì™„ë£Œ: {len(seed_titles)}ê°œ")
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
        
        # [2ë‹¨ê³„] í‚¤ì›Œë“œ ì¶”ì¶œ
        top_keywords = ai_engine.extract_top_entities(category, seed_titles)
        if not top_keywords: continue
            
        print(f"   ğŸ’ ì¶”ì¶œëœ ë­í‚¹: {', '.join(top_keywords[:5])}...")

        # [3ë‹¨ê³„] í‚¤ì›Œë“œë³„ ì‹¬ì¸µ ë¶„ì„
        category_news_list = []
        target_keywords = top_keywords[:TOP_RANK_LIMIT]
        
        for rank, kw in enumerate(target_keywords):
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ìš”ì•½ ì¤‘...")
            
            try:
                raw_articles = crawler.get_naver_api_news(kw, display=10)
                if not raw_articles: continue

                full_contents = []
                main_image = None
                
                # ìƒìœ„ 5ê°œ ê¸°ì‚¬ í™•ì¸
                for art in raw_articles[:5]:
                    # ğŸš¨ [í•µì‹¬] get_article_dataì— í‚¤ì›Œë“œë¥¼ ë„˜ê²¨ì„œ ê²€ì¦ì‹œí‚´
                    text, img = crawler.get_article_data(art['link'], target_keyword=kw)
                    
                    if text: 
                        full_contents.append(text)
                    
                    if not main_image and img:
                        if img.startswith("http://"):
                            img = img.replace("http://", "https://")
                        main_image = img

                # ìœ íš¨í•œ ë³¸ë¬¸ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€ (ì“°ë ˆê¸° ìš”ì•½ ë°©ì§€)
                if not full_contents:
                    print(f"      â˜ï¸ '{kw}': ê´€ë ¨ ë³¸ë¬¸ ì—†ìŒ (Skip)")
                    continue

                # AI ìš”ì•½ ìˆ˜í–‰
                briefing = ai_engine.synthesize_briefing(kw, full_contents)
                
                # AIê°€ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ë‹µí–ˆìœ¼ë©´ ì €ì¥ ì•ˆ í•¨
                if "No specific news" in briefing:
                     print(f"      â˜ï¸ '{kw}': AIê°€ ìš”ì•½í•  ì •ë³´ê°€ ì—†ë‹¤ê³  íŒë‹¨í•¨.")
                     continue

                final_img = main_image or f"https://placehold.co/600x400/111/cyan?text={kw}"

                news_item = {
                    "category": category,
                    "rank": rank + 1,
                    "keyword": kw,
                    "title": f"[{kw}] Key Trends & Issues",
                    "summary": briefing,
                    "link": None,            # ë§í¬ X
                    "image_url": final_img,  # ì´ë¯¸ì§€ O (HTTPS)
                    "score": 10.0 - (rank * 0.1),
                    "likes": 0, "dislikes": 0,
                    "created_at": datetime.now().isoformat(),
                    "published_at": datetime.now().isoformat()
                }
                category_news_list.append(news_item)
                time.sleep(0.5)
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # [4ë‹¨ê³„] ì €ì¥
        if category_news_list:
            repository.save_to_archive(category_news_list[:10])
            repository.refresh_live_news(category, category_news_list)

    print("\nğŸ‰ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

if __name__ == "__main__":
    run_master_scraper()

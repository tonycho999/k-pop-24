import os
import sys
import json
import time
import requests
import re
from supabase import create_client, Client
from datetime import datetime, timedelta
from dateutil.parser import isoparse 
from dotenv import load_dotenv
from groq import Groq
from bs4 import BeautifulSoup

load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

# [ê¸°ì¡´] RLS ë¬¸ì œ ì—†ì´ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰
supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY") or os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")

if not supabase_url or not supabase_key:
    print("ğŸš¨ ì˜¤ë¥˜: .env íŒŒì¼ì— Supabase URL ë˜ëŠ” Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

supabase: Client = create_client(supabase_url, supabase_key)
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

CATEGORY_MAP = {
    "k-pop": ["ì»´ë°±", "ë¹Œë³´ë“œ", "ì•„ì´ëŒ", "ë®¤ì§", "ë¹„ë””ì˜¤", "ì±Œë¦°ì§€", "í¬í† ì¹´ë“œ", "ì›”ë“œíˆ¬ì–´", "ê°€ìˆ˜"],
    "k-drama": ["ë“œë¼ë§ˆ", "ì‹œì²­ë¥ ", "ë„·í”Œë¦­ìŠ¤", "OTT", "ë°°ìš°", "ìºìŠ¤íŒ…", "ëŒ€ë³¸ë¦¬ë”©", "ì¢…ì˜"],
    "k-movie": ["ì˜í™”", "ê°œë´‰", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "ì‹œì‚¬íšŒ", "ì˜í™”ì œ", "ê´€ê°", "ë¬´ëŒ€ì¸ì‚¬"],
    "k-entertain": ["ì˜ˆëŠ¥", "ìœ íŠœë¸Œ", "ê°œê·¸ë§¨", "ì½”ë¯¸ë””ì–¸", "ë°©ì†¡", "ê°œê·¸ìš°ë¨¼"],
    "k-culture": ["í‘¸ë“œ", "ë·°í‹°", "ì›¹íˆ°", "íŒì—…ìŠ¤í† ì–´", "íŒ¨ì…˜", "ìŒì‹", "í•´ì™¸ë°˜ì‘"]
}

# [ê¸°ì¡´] AI ëª¨ë¸ ë™ì  ì¡°íšŒ
def get_best_model():
    try:
        models_raw = groq_client.models.list()
        available_models = [m.id for m in models_raw.data]
        
        def model_scorer(model_id):
            score = 0
            model_id = model_id.lower()
            if "llama" in model_id: score += 1000
            elif "mixtral" in model_id: score += 500
            elif "gemma" in model_id: score += 100
            
            version_match = re.search(r'(\d+\.?\d*)', model_id)
            if version_match:
                try:
                    version = float(version_match.group(1))
                    score += version * 100 
                except: pass

            if "70b" in model_id: score += 50
            elif "8b" in model_id: score += 10
            if "versatile" in model_id: score += 5
            return score

        available_models.sort(key=model_scorer, reverse=True)
        print(f"ğŸ¤– AI ëª¨ë¸ ìš°ì„ ìˆœìœ„: {available_models[:3]}")
        return available_models

    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        return ["llama-3.3-70b-versatile", "mixtral-8x7b-32768"]

MODELS_TO_TRY = get_best_model()

def get_naver_api_news(keyword):
    import urllib.parse, urllib.request
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

def get_article_image(link):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        res = requests.get(link, headers=headers, timeout=3)
        soup = BeautifulSoup(res.text, 'html.parser')
        candidates = []

        main_content = soup.select_one('#dic_area, #articleBodyContents, .article_view, #articeBody, .news_view')
        if main_content:
            imgs = main_content.find_all('img')
            for i in imgs:
                src = i.get('src') or i.get('data-src')
                if src and 'http' in src:
                    width = i.get('width')
                    if width and width.isdigit() and int(width) < 200: continue
                    candidates.append(src)

        og = soup.find('meta', property='og:image')
        if og and og.get('content'): candidates.append(og['content'])

        for img_url in candidates:
            bad_keywords = r'logo|icon|button|share|banner|thumb|profile|default|ranking|news_stand|ssl.pstatic.net'
            if re.search(bad_keywords, img_url, re.IGNORECASE): continue
            return img_url
        return None
    except: return None

# [ê¸°ì¡´] ë‰´ìŠ¤ ìš”ì•½ 20~40%
def ai_category_editor(category, news_batch):
    if not news_batch: return []
    limited_batch = news_batch[:50]
    
    raw_text = ""
    for i, n in enumerate(limited_batch):
        clean_desc = n['description'].replace('<b>', '').replace('</b>', '').replace('&quot;', '"')
        raw_text += f"[{i}] Title: {n['title']} / Context: {clean_desc}\n"
    
    prompt = f"""
    Task: Select highly relevant news items for '{category}'. 
    Target Quantity: Try to select up to 30 items if they are relevant.
    
    Constraints: 
    1. English Title: Translate naturally.
    2. English Summary: 
       - Write a DETAILED narrative summary (approx. 20-40% length of a typical article).
       - DO NOT use bullet points. Write 5-8 sentences in a cohesive paragraph.
       - Include Who, When, Where, Why based on the context.
    3. AI Score (0.0-10.0): Judge based on importance and trendiness.
    4. Return JSON format strictly.

    News List:
    {raw_text}

    Output JSON Format:
    {{
        "articles": [
            {{ "original_index": 0, "eng_title": "...", "summary": "Detailed summary...", "score": 8.5 }}
        ]
    }}
    """
    
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(
                messages=[{"role": "system", "content": f"You are a K-Enter Journalist for {category}."},
                          {"role": "user", "content": prompt}], 
                model=model, 
                response_format={"type": "json_object"}
            )
            data = json.loads(res.choices[0].message.content)
            articles = data.get('articles', [])
            if articles: return articles
        except Exception as e:
            print(f"      âš ï¸ {model} ì˜¤ë¥˜ ({str(e)[:60]}...). ë‹¤ìŒ ëª¨ë¸ ì‹œë„.")
            continue
    return []

# [ê¸°ì¡´] í‚¤ì›Œë“œ ë¶„ì„
def update_hot_keywords():
    print("ğŸ“Š AI í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
    res = supabase.table("live_news").select("title").order("created_at", desc=True).limit(100).execute()
    titles = [item['title'] for item in res.data]
    if not titles:
        print("   âš ï¸ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    titles_text = "\n".join([f"- {t}" for t in titles])
    prompt = f"""
    Analyze the following K-Entertainment news titles and identify the TOP 10 most trending keywords.
    [Rules]
    1. Extract specific Entities: Person Name (e.g., "Lee Min-ho", NOT "Lee"), Group Name (e.g., "BTS"), Drama/Movie Title (e.g., "Squid Game").
    2. Merge related concepts: If "Jin" and "BTS" are both popular, use "BTS Jin".
    3. EXCLUDE generic words: Do NOT use words like "Variety", "Actor", "K-pop", "Review", "Netizens", "Update", "Official", "Comeback", "Teaser".
    4. Return JSON format with 'keyword' and estimated 'count' (importance score 1-100).
    [Titles]
    {titles_text}
    [Output Format JSON]
    {{
        "keywords": [
            {{ "keyword": "BTS Jin", "count": 95, "rank": 1 }},
            {{ "keyword": "Squid Game 2", "count": 80, "rank": 2 }}
        ]
    }}
    """
    
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(
                messages=[{"role": "system", "content": "You are a K-Trend Analyst."},
                          {"role": "user", "content": prompt}], 
                model=model, 
                response_format={"type": "json_object"}
            )
            result = json.loads(res.choices[0].message.content)
            keywords = result.get('keywords', [])
            
            if not keywords: continue
            
            print(f"   ğŸ”¥ AIê°€ ì¶”ì¶œí•œ ì§„ì§œ íŠ¸ë Œë“œ: {[k.get('keyword') for k in keywords[:5]]}...")
            
            supabase.table("trending_keywords").delete().neq("id", 0).execute()
            
            insert_data = []
            for i, item in enumerate(keywords):
                insert_data.append({
                    "keyword": item.get('keyword'),
                    "count": item.get('count', 0),
                    "rank": item.get('rank', i + 1), 
                    "updated_at": datetime.now().isoformat()
                })
            
            if insert_data:
                supabase.table("trending_keywords").insert(insert_data).execute()
                print("   âœ… í‚¤ì›Œë“œ ë­í‚¹ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                return 

        except Exception as e:
            print(f"      âš ï¸ {model} ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue

# [ì‹ ê·œ ì¶”ê°€] ìƒìœ„ ë­í¬ ê¸°ì‚¬ ì•„ì¹´ì´ë¹™ í•¨ìˆ˜
def archive_top_articles():
    print("ğŸ—„ï¸ ìƒìœ„ ë­í¬(Top 10) ê¸°ì‚¬ ì•„ì¹´ì´ë¹™ ì‹œì‘...")
    
    for category in CATEGORY_MAP.keys():
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ rankê°€ 1~10ë“±ì¸ ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜´ (score ë†’ì€ ìˆœë„ ê°€ëŠ¥)
        res = supabase.table("live_news").select("*").eq("category", category).order("rank", ascending=True).limit(10).execute()
        top_articles = res.data
        
        if top_articles:
            # search_archive í…Œì´ë¸”ì— ì €ì¥ (ì¤‘ë³µëœ linkê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸)
            # ì£¼ì˜: search_archive í…Œì´ë¸”ì´ ì¡´ì¬í•´ì•¼ í•¨
            try:
                supabase.table("search_archive").upsert(top_articles, on_conflict="link").execute()
                print(f"   ğŸ’¾ {category.upper()}: Top {len(top_articles)}ê°œ -> ì•„ì¹´ì´ë¸Œ ì €ì¥ ì™„ë£Œ.")
            except Exception as e:
                print(f"   âš ï¸ ì•„ì¹´ì´ë¸Œ ì €ì¥ ì‹¤íŒ¨ ({category}): {e}")

def run():
    print("ğŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™ (30ê°œ ì‚¬ìˆ˜ + ì•„ì¹´ì´ë¹™ + ë™ì  AI)...")
    
    for category, keywords in CATEGORY_MAP.items():
        print(f"ğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

        # 1. ìˆ˜ì§‘
        raw_news = []
        for kw in keywords: raw_news.extend(get_naver_api_news(kw))
        
        # 2. ì¤‘ë³µ ì œê±°
        db_res = supabase.table("live_news").select("link").eq("category", category).execute()
        existing_links = {item['link'] for item in db_res.data}
        
        new_candidate_news = []
        seen_links = set()
        for n in raw_news:
            if n['link'] not in existing_links and n['link'] not in seen_links:
                new_candidate_news.append(n)
                seen_links.add(n['link'])
        
        print(f"   ğŸ” ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ê¸°ì¡´ DB ì¤‘ë³µ ì œì™¸: {len(new_candidate_news)}ê°œ")

        # 3. AI ì„ ë³„
        selected = ai_category_editor(category, new_candidate_news)
        print(f"   ã„´ AI ì„ ë³„ ì™„ë£Œ: {len(selected)}ê°œ")

        # 4. ì‹ ê·œ ë‰´ìŠ¤ ì €ì¥
        if selected:
            new_data_list = []
            for i, art in enumerate(selected):
                idx = art.get('original_index')
                if idx is None or idx >= len(new_candidate_news): continue
                
                orig = new_candidate_news[idx]
                img = get_article_image(orig['link']) or f"https://placehold.co/600x400/111/cyan?text={category}"

                new_data_list.append({
                    "rank": art.get('rank', 99), 
                    "category": category, 
                    "title": art.get('eng_title', orig['title']),
                    "summary": art.get('summary', 'Detailed summary not available.'), 
                    "link": orig['link'], 
                    "image_url": img,
                    "score": art.get('score', 5.0), 
                    "likes": 0, 
                    "dislikes": 0, 
                    "created_at": datetime.now().isoformat()
                })
            
            if new_data_list:
                supabase.table("live_news").upsert(new_data_list, on_conflict="link").execute()
                print(f"   âœ… ì‹ ê·œ {len(new_data_list)}ê°œ DB ì €ì¥ ì™„ë£Œ.")

        # [ì¡°ê±´ 5 & 6] ìŠ¤ë§ˆíŠ¸ ì‚­ì œ ë¡œì§ (ë¬´ì¡°ê±´ 30ê°œ ìœ ì§€)
        res = supabase.table("live_news").select("id", "created_at", "score").eq("category", category).execute()
        all_articles = res.data
        total_count = len(all_articles)
        
        print(f"   ğŸ“Š í˜„ì¬ DB ì´ ê°œìˆ˜: {total_count}ê°œ (ëª©í‘œ: 30ê°œ ìœ ì§€)")

        if total_count > 30:
            delete_ids = []
            
            # ì „ëµ A: 24ì‹œê°„ ì§€ë‚œ ê¸°ì‚¬ ì‚­ì œ
            now = datetime.now()
            threshold = now - timedelta(hours=24)
            
            try:
                all_articles.sort(key=lambda x: isoparse(x['created_at']).replace(tzinfo=None))
            except: pass

            remaining_count = total_count
            
            for art in all_articles:
                try: art_date = isoparse(art['created_at']).replace(tzinfo=None)
                except: art_date = datetime(2000, 1, 1)

                if art_date < threshold:
                    if remaining_count > 30:
                        delete_ids.append(art['id'])
                        remaining_count -= 1
                    else: break

            # ì „ëµ B: ì ìˆ˜ ë‚®ì€ ìˆœ ì‚­ì œ
            if remaining_count > 30:
                survivors = [a for a in all_articles if a['id'] not in delete_ids]
                survivors.sort(key=lambda x: x['score'])
                
                for art in survivors:
                    if remaining_count > 30:
                        delete_ids.append(art['id'])
                        remaining_count -= 1
                    else: break

            if delete_ids:
                supabase.table("live_news").delete().in_("id", delete_ids).execute()
                print(f"   ğŸ§¹ ê³µê°„ í™•ë³´: {len(delete_ids)}ê°œ ì‚­ì œ ì™„ë£Œ (í˜„ì¬ {remaining_count}ê°œ ìœ ì§€).")

    # [ë§ˆì§€ë§‰ ë‹¨ê³„] ì•„ì¹´ì´ë¹™ ë° í‚¤ì›Œë“œ ë¶„ì„
    archive_top_articles() # [ì¶”ê°€ëœ í•¨ìˆ˜ í˜¸ì¶œ]
    update_hot_keywords()
    
    print(f"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ.")

if __name__ == "__main__":
    run()

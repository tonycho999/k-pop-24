import sys
import os

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í´ë”ë¥¼ pathì— ì¶”ê°€ (ëª¨ë“ˆ import ë¬¸ì œ ë°©ì§€)
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import time
from datetime import datetime
from dotenv import load_dotenv

# ëª¨ë“ˆ import
from scraper.config import CATEGORY_MAP
from scraper import crawler, ai_engine, repository, update_rankings

load_dotenv()

def run_scraper():
    print("ğŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™ (Rules 1-6 Applied)...")
    
    for category, keywords in CATEGORY_MAP.items():
        try:
            print(f"\nğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

            # [ê·œì¹™ 1] ìˆ˜ì§‘ (ìµœì‹ ìˆœ ì •ë ¬ë¨)
            raw_news = []
            for kw in keywords: 
                raw_news.extend(crawler.get_naver_api_news(kw))
            
            # [ê·œì¹™ 2] ì¤‘ë³µ ì œê±°
            existing_links = repository.get_existing_links(category)
            
            new_candidate_news = []
            seen_links = set()
            for n in raw_news:
                if n['link'] not in existing_links and n['link'] not in seen_links:
                    new_candidate_news.append(n)
                    seen_links.add(n['link'])
            
            print(f"    ğŸ” ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ì¤‘ë³µ ì œê±° í›„: {len(new_candidate_news)}ê°œ")

            if not new_candidate_news:
                continue

            # [ê·œì¹™ 3] ìµœì‹  ê¸°ì‚¬ 70ê°œ ì„ ì • -> AI í‰ê°€
            ai_input_news = new_candidate_news[:70]

            # ğŸŸ¢ [í•µì‹¬] AI ìš”ì•½ í’ˆì§ˆì„ ìœ„í•´ ë³¸ë¬¸ í¬ë¡¤ë§ (1,500ì í™•ë³´)
            print(f"    ğŸ•·ï¸ AI ë¶„ì„ì„ ìœ„í•œ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ ({len(ai_input_news)}ê°œ)...")
            for news_item in ai_input_news:
                # crawler.pyì˜ get_article_data í˜¸ì¶œ
                full_text, image_url = crawler.get_article_data(news_item['link'])
                
                # ë³¸ë¬¸(full_text)ì€ AI ìš”ì•½ìš©, ì´ë¯¸ì§€(image_url)ëŠ” ì €ì¥ìš©
                news_item['full_content'] = full_text  
                news_item['crawled_image'] = image_url 

            # AI ì„ ë³„ (ì ìˆ˜ ë¶€ì—¬ ë° 3ë‹¨ê³„ ìš”ì•½)
            # ì´ì œ ai_input_news ì•ˆì— 'full_content'ê°€ ìˆìœ¼ë¯€ë¡œ AIëŠ” ì´ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½í•¨
            analyzed_list = ai_engine.ai_category_editor(category, ai_input_news)
            print(f"    ã„´ AI ë¶„ì„ ì™„ë£Œ: {len(analyzed_list)}ê°œ")

            if analyzed_list:
                # [ê·œì¹™ 3 í›„ë°˜] ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ 30ê°œ ì„ ì •
                # ì ìˆ˜(score) ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                analyzed_list.sort(key=lambda x: x.get('score', 0), reverse=True)
                
                # ìƒìœ„ 30ê°œë§Œ ìë¥´ê¸° (ê·œì¹™ 4: ìƒˆë¡œìš´ ê¸°ì‚¬ 30ê°œ ì €ì¥)
                top_30_news = analyzed_list[:30]
                
                new_data_list = []
                for art in top_30_news:
                    idx = art.get('original_index')
                    if idx is None or idx >= len(ai_input_news): continue
                    
                    orig = ai_input_news[idx]
                    
                    # ì´ë¯¸ ìœ„ì—ì„œ ê¸ì–´ì˜¨ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ placeholder ì‚¬ìš©
                    img = orig.get('crawled_image') or f"https://placehold.co/600x400/111/cyan?text={category}"

                    # DB ì €ì¥ìš© ê°ì²´ ìƒì„±
                    news_item = {
                        "category": category, 
                        "title": art.get('eng_title', orig['title']),
                        "summary": art.get('summary', 'Summary not available.'), 
                        "link": orig['link'], 
                        "image_url": img,
                        "score": art.get('score', 5.0), 
                        "likes": 0, 
                        "dislikes": 0, 
                        "created_at": datetime.now().isoformat(),
                        "published_at": orig.get('published_at', datetime.now()).isoformat()
                    }
                    new_data_list.append(news_item)
                
                # [ê·œì¹™ 4] DB ì €ì¥ (30ê°œ) + [ì•„ì¹´ì´ë¹™ ë¡œì§ í¬í•¨]
                repository.save_news(new_data_list)

            # [ê·œì¹™ 5 & 6] ìŠ¬ë¡¯ ê´€ë¦¬ (ì „ì²´ 30ê°œ ìœ ì§€, ì‹œê°„/ì ìˆ˜ ì‚­ì œ)
            repository.manage_slots(category)

        except Exception as e:
            print(f"âš ï¸ Error processing category {category}: {e}")
            continue

    # í‚¤ì›Œë“œ ë¶„ì„ (ì˜µì…˜ - í•¨ìˆ˜ê°€ ì¡´ì¬í•  ë•Œë§Œ ì‹¤í–‰)
    try:
        print("\nğŸ“Š AI í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        titles = repository.get_recent_titles()
        if titles and hasattr(ai_engine, 'ai_analyze_keywords'):
            keywords = ai_engine.ai_analyze_keywords(titles)
            if keywords:
                repository.update_keywords_db(keywords)
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    print("ğŸ‰ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ.")

def main():
    print("ğŸš€ K-Enter AI News Bot Started...")
    
    # ìˆœìœ„ ì—…ë°ì´íŠ¸
    try:
        update_rankings.update_rankings() 
    except Exception as e:
        print(f"âš ï¸ ìˆœìœ„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
    run_scraper()
    
    print("âœ… All Tasks Completed.")

if __name__ == "__main__":
    main()

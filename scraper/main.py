import os
import json
import time
import requests
from supabase import create_client, Client
from dotenv import load_dotenv
from ddgs import DDGS

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. Supabase ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. Gemini API í‚¤ ì„¤ì •
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

if GOOGLE_API_KEY:
    print(f"ğŸ”‘ API Key ë¡œë“œ ì™„ë£Œ: {GOOGLE_API_KEY[:5]}...")
else:
    print("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤!")

# âœ… [ìˆ˜ì • 1] K-Variety -> K-Entertainìœ¼ë¡œ ë³€ê²½ (DB ì €ì¥ ì´ë¦„ë„ ë°”ë€œ)
CATEGORIES = {
    "K-Pop": "k-pop latest news trends",
    "K-Drama": "k-drama ratings news",
    "K-Movie": "korean movie box office news",
    "K-Entertain": "korean variety show news reality show trends", 
    "K-Culture": "seoul travel food trends"
}

def get_dynamic_model_url():
    list_url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GOOGLE_API_KEY}"
    try:
        response = requests.get(list_url)
        if response.status_code != 200:
            return "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
        data = response.json()
        models = data.get('models', [])
        valid_models = []
        for m in models:
            name = m['name'] 
            methods = m.get('supportedGenerationMethods', [])
            if 'generateContent' in methods and 'flash' in name:
                valid_models.append(name)
        if valid_models:
            print(f"âœ… ìµœì  ëª¨ë¸ ë°œê²¬: {valid_models[-1]}")
            return f"https://generativelanguage.googleapis.com/v1beta/{valid_models[-1]}:generateContent"
        return "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
    except Exception:
        return "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

CURRENT_MODEL_URL = get_dynamic_model_url()

def get_fallback_image(keyword):
    """ë‰´ìŠ¤ì— ì´ë¯¸ì§€ê°€ ì—†ì„ ë•Œ, ì´ë¯¸ì§€ ê²€ìƒ‰ì„ í†µí•´ ê°•ì œë¡œ ì°¾ì•„ë‚´ëŠ” í•¨ìˆ˜"""
    try:
        with DDGS() as ddgs:
            imgs = list(ddgs.images(keywords=keyword, region="kr-kr", safesearch="off", max_results=1))
            if imgs and len(imgs) > 0:
                return imgs[0].get('image')
    except Exception:
        return ""
    return ""

def search_web(keyword):
    """DuckDuckGo ê²€ìƒ‰: HTTPSë§Œ ìˆ˜ì§‘ + ì´ë¯¸ì§€ í•„ìˆ˜ + ë‚´ìš© ì¶©ì‹¤"""
    print(f"ğŸ” [Search] '{keyword}' ê²€ìƒ‰ ì¤‘...")
    results = []
    
    try:
        with DDGS() as ddgs:
            # 1. ë‰´ìŠ¤ ê²€ìƒ‰
            ddg_results = list(ddgs.news(query=keyword, region="kr-kr", safesearch="off", max_results=15))
            
            for r in ddg_results:
                title = r.get('title', '')
                body = r.get('body', r.get('snippet', ''))
                link = r.get('url', r.get('href', ''))
                image = r.get('image', r.get('thumbnail', ''))

                # [í•„ìˆ˜] ì œëª©, ë³¸ë¬¸, HTTPS ë§í¬ ì²´í¬
                if not title or not body or not link or not link.startswith("https"):
                    continue

                # âœ… ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ -> ë³„ë„ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
                if not image:
                    image = get_fallback_image(title)
                    time.sleep(0.5) 

                # âœ… ê·¸ë˜ë„ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´? ê³¼ê°íˆ ë²„ë¦¼ (ì´ë¯¸ì§€ í•„ìˆ˜ ì •ì±…)
                if not image:
                    continue

                results.append(f"ì œëª©: {title}\në‚´ìš©: {body}\në§í¬: {link}\nì´ë¯¸ì§€: {image}")
                
    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {e}")
    
    return "\n\n".join(results)

def call_gemini_api(category_name, raw_data):
    print(f"ğŸ¤– [Gemini] '{category_name}' ê¸°ì‚¬ ì‘ì„± ì¤‘ (20ë…„ì°¨ ë² í…Œë‘ ëª¨ë“œ)...")
    
    headers = {"Content-Type": "application/json"}
    
    # âœ… [ìˆ˜ì • 2] ë² í…Œë‘ ê¸°ì í”„ë¡¬í”„íŠ¸ + ê¸€ììˆ˜ ì œí•œ (100~500ì)
    prompt = f"""
    [Role]
    You are a veteran K-Entertainment journalist with 20 years of experience.
    Your writing style is analytical, insightful, and engaging. You provide context, not just facts.

    [Input Data]
    {raw_data[:20000]} 

    [Task]
    Select the Top 10 most impactful news items for '{category_name}' and rewrite them.
    
    [Content Requirements - STRICT]
    1. **Length**: Each summary MUST be between **100 and 500 characters** (Korean). Not too short, not too long.
    2. **Depth**: Include the background of the event or the public's reaction. Explain WHY this is important.
    3. **Tone**: Professional journalistic tone (e.g., "~í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤", "~ì— ì´ëª©ì´ ì§‘ì¤‘ëœë‹¤").
    4. **Image**: You MUST map the 'image_url' from the raw data exactly.

    [Output Format (JSON Only)]
    {{
      "news_updates": [
        {{ 
          "keyword": "Main Subject", 
          "title": "Compelling Title (Korean)", 
          "summary": "Detailed Article (Korean, 100-500 chars)", 
          "link": "Original Link",
          "image_url": "URL starting with https"
        }}
      ],
      "rankings": [
        {{ "rank": 1, "title": "Name", "meta": "Short Info", "score": 98 }}
      ]
    }}
    """
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        full_url = f"{CURRENT_MODEL_URL}?key={GOOGLE_API_KEY}"
        response = requests.post(full_url, headers=headers, json=payload)
        
        if response.status_code == 200:
            try:
                text = response.json()['candidates'][0]['content']['parts'][0]['text']
                text = text.replace("```json", "").replace("```", "").strip()
                return json.loads(text)
            except Exception as e:
                print(f"   âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return None
        elif response.status_code == 429:
            print(f"   âŒ API í•œë„ ì´ˆê³¼ (429): ì ì‹œ ëŒ€ê¸° í•„ìš”")
            return None
        elif response.status_code == 503:
             print(f"   âŒ ì„œë²„ ê³¼ë¶€í•˜ (503): ì ì‹œ ëŒ€ê¸° í•„ìš”")
             return None
        else:
            print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨ ({response.status_code}): {response.text[:200]}")
            return None
    except Exception as e:
        print(f"   âŒ ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def update_database(category, data):
    # ë‰´ìŠ¤ ì €ì¥
    news_list = data.get("news_updates", [])
    if news_list:
        clean_news = []
        for item in news_list:
            if not item.get("image_url"): continue

            summary = item.get("summary", "")
            
            # (ì˜µì…˜) í˜¹ì‹œë¼ë„ ë„ˆë¬´ ì§§ìœ¼ë©´ ì €ì¥ ì•ˆ í•˜ê±°ë‚˜ ì ìˆ˜ ê¹ìŒ
            if len(summary) < 50: 
                print(f"   âš ï¸ ê¸°ì‚¬ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ ({len(summary)}ì). ê±´ë„ˆëœ€.")
                continue

            clean_news.append({
                "category": category,
                "keyword": item.get("keyword", category),
                "title": item.get("title", "ì œëª© ì—†ìŒ"),
                "summary": summary,
                "link": item.get("link", ""),
                "image_url": item.get("image_url"),
                "created_at": "now()",
                "likes": 0,
                "score": 80 + (len(summary) / 10) # ê¸´ ê¸€ì¼ìˆ˜ë¡ ì ìˆ˜ ë†’ê²Œ ì±…ì •
            })
        
        if clean_news:
            try:
                supabase.table("live_news").upsert(clean_news, on_conflict="category,keyword,title").execute()
                supabase.table("search_archive").upsert(clean_news, on_conflict="category,keyword,title").execute()
                print(f"   ğŸ’¾ ë‰´ìŠ¤ {len(clean_news)}ê±´ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ë­í‚¹ ì €ì¥ (live_rankings)
    rank_list = data.get("rankings", [])
    if rank_list:
        clean_ranks = []
        for item in rank_list:
            clean_ranks.append({
                "category": category,
                "rank": item.get("rank"),
                "title": item.get("title"),
                "meta_info": item.get("meta", ""),
                "score": item.get("score", 0),
                "updated_at": "now()"
            })
        try:
            supabase.table("live_rankings").upsert(clean_ranks, on_conflict="category,rank").execute()
            print(f"   ğŸ† ë­í‚¹ ê°±ì‹  ì™„ë£Œ")
        except Exception as e:
             print(f"   âš ï¸ ë­í‚¹ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    print(f"ğŸš€ ìŠ¤í¬ë˜í¼ ì‹œì‘ (Veteran Journalist Mode)")
    for category, search_keyword in CATEGORIES.items():
        raw_text = search_web(search_keyword)
        
        if len(raw_text) < 50: 
            print(f"âš ï¸ {category} : ë‰´ìŠ¤ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€")
            continue

        data = call_gemini_api(category, raw_text)
        if data:
            update_database(category, data)
        
        # 429 ì—ëŸ¬ ë°©ì§€ìš© ëŒ€ê¸°
        print("â³ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì „ 15ì´ˆ ëŒ€ê¸°...")
        time.sleep(15) 

    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()

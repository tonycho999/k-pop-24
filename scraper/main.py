import sys
import os

# ëª¨ë“ˆ import ë¬¸ì œ ë°©ì§€
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import time
from datetime import datetime
from dotenv import load_dotenv

# í•„ìˆ˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from scraper import crawler, ai_engine, repository, update_rankings
from scraper.config import CATEGORY_MAP # ë°±ì—…ìš© ê³ ì • í‚¤ì›Œë“œ ì„í¬íŠ¸

load_dotenv()

def run_master_scraper():
    print("ğŸš€ êµ¬ê¸€ íŠ¸ë Œë“œ ê¸°ë°˜ 9ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™...")
    
    # [1ë‹¨ê³„] êµ¬ê¸€ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘
    raw_trending_keywords = crawler.get_google_trending_keywords()
    
    categorized_keywords = {}

    if raw_trending_keywords:
        print(f"ğŸ”¥ êµ¬ê¸€ íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(raw_trending_keywords)}ê°œ ìˆ˜ì§‘ ì„±ê³µ. AI ë¶„ë¥˜ ì‹œì‘...")
        # [2~3ë‹¨ê³„] AI ë¶„ë¥˜ ë° ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì„ ì •
        categorized_keywords = ai_engine.ai_filter_and_rank_keywords(raw_trending_keywords)
        
        if not categorized_keywords:
            print("âš ï¸ AI ë¶„ë¥˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê³ ì • í‚¤ì›Œë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            categorized_keywords = CATEGORY_MAP
    else:
        # [Fallback] êµ¬ê¸€ íŠ¸ë Œë“œ ì‹¤íŒ¨ ì‹œ ê³ ì • í‚¤ì›Œë“œ ì‚¬ìš©
        print("âš ï¸ êµ¬ê¸€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨. ì„¤ì • íŒŒì¼ì˜ [ê³ ì • í‚¤ì›Œë“œ]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ê³„ì†í•©ë‹ˆë‹¤.")
        categorized_keywords = CATEGORY_MAP

    # ì¹´í…Œê³ ë¦¬ë³„ ë£¨í”„ ì‹œì‘
    for category, keywords in categorized_keywords.items():
        try:
            print(f"\nğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘ (í‚¤ì›Œë“œ: {keywords})")

            # [4ë‹¨ê³„] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
            raw_news = []
            for kw in keywords: 
                raw_news.extend(crawler.get_naver_api_news(kw))
            
            # [5ë‹¨ê³„] DB ì¤‘ë³µ ì²´í¬
            existing_links = repository.get_existing_links(category)
            # 70ê°œê¹Œì§€ë§Œ í›„ë³´ ì„ ì •
            new_candidate_news = [n for n in raw_news if n['link'] not in existing_links][:70]

            if not new_candidate_news:
                print(f"    âœ¨ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # ë³¸ë¬¸ 1,500ì ë° ì´ë¯¸ì§€ í™•ë³´
            print(f"    ğŸ•·ï¸ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ ({len(new_candidate_news)}ê°œ)...")
            for news_item in new_candidate_news:
                full_text, image_url = crawler.get_article_data(news_item['link'])
                news_item['full_content'] = full_text  
                news_item['crawled_image'] = image_url 

            # [6ë‹¨ê³„] 3ì¤‘ AI ì—”ì§„ì„ ì´ìš©í•œ í‰ì  ë° 3ë‹¨ê³„ ìš”ì•½
            analyzed_list = ai_engine.ai_category_editor(category, new_candidate_news)
            
            if analyzed_list:
                # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 30ê°œ ì„ ì •
                analyzed_list.sort(key=lambda x: x.get('score', 0), reverse=True)
                top_30_news = analyzed_list[:30]
                
                # [7ë‹¨ê³„] DB ì €ì¥
                new_data_list = []
                for art in top_30_news:
                    idx = art.get('original_index')
                    if idx is not None and idx < len(new_candidate_news):
                        orig = new_candidate_news[idx]
                        
                        # ì´ë¯¸ì§€ URL ê²°ì • (í¬ë¡¤ë§ëœ ì´ë¯¸ì§€ ìš°ì„ , ì—†ìœ¼ë©´ placeholder)
                        final_img = orig.get('crawled_image')
                        if not final_img:
                             final_img = f"https://placehold.co/600x400/111/cyan?text={category}"

                        new_data_list.append({
                            "category": category, 
                            "title": art.get('eng_title', orig['title']),
                            "summary": art.get('summary', 'Summary not available.'), 
                            "link": orig['link'], 
                            "image_url": final_img,
                            "score": art.get('score', 5.0), 
                            "likes": 0, "dislikes": 0, 
                            "created_at": datetime.now().isoformat(),
                            "published_at": orig.get('published_at', datetime.now()).isoformat()
                        })
                
                if new_data_list:
                    repository.save_news(new_data_list)

            # [8~9ë‹¨ê³„] ìŠ¬ë¡¯ ê´€ë¦¬
            repository.manage_slots(category)

        except Exception as e:
            print(f"âš ï¸ {category} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    print("\nğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì™„ë£Œ.")

def main():
    print("ğŸš€ K-Enter AI News Bot Master Mode Started...")
    try: update_rankings.update_rankings() 
    except: pass
    
    run_master_scraper()

if __name__ == "__main__":
    main()

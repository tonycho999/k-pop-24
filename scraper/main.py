import sys
import os
import time
from datetime import datetime
from dotenv import load_dotenv

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS

load_dotenv()

# ì´ 30ìœ„ê¹Œì§€ ë¶„ì„
TARGET_RANK_LIMIT = 30 

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # [1ë‹¨ê³„] ì”¨ì•— ìˆ˜ì§‘
        seed_titles = []
        try:
            for seed in seeds:
                news = crawler.get_naver_api_news(seed, display=20)
                seed_titles.extend([n['title'] for n in news])
            seed_titles = list(set(seed_titles))
            print(f"   ğŸŒ± ì›ì„ ìˆ˜ì§‘ ì™„ë£Œ: {len(seed_titles)}ê°œ")
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            continue
        
        # [2ë‹¨ê³„] í‚¤ì›Œë“œ ì¶”ì¶œ (ì‚¬ëŒ/ì‘í’ˆ ë¶„ë¥˜ í¬í•¨)
        # top_entitiesëŠ” [{'keyword': 'BTS', 'type': 'person'}, ...] í˜•íƒœ
        top_entities = ai_engine.extract_top_entities(category, seed_titles)
        if not top_entities: continue
            
        print(f"   ğŸ’ ì¶”ì¶œëœ í‚¤ì›Œë“œ (Top 5): {', '.join([e['keyword'] for e in top_entities[:5]])}...")

        # [3ë‹¨ê³„] í‚¤ì›Œë“œë³„ ì‹¬ì¸µ ë¶„ì„ (30ìœ„ê¹Œì§€)
        category_news_list = []
        
        # 30ê°œê¹Œì§€ë§Œ ì²˜ë¦¬
        target_list = top_entities[:TARGET_RANK_LIMIT]
        
        for rank, entity in enumerate(target_list):
            kw = entity.get('keyword')
            k_type = entity.get('type', 'content') # ê¸°ë³¸ê°’ content
            
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ({k_type}) ì²˜ë¦¬ ì¤‘...")
            
            try:
                # 3-1. ê¸°ì‚¬ ê²€ìƒ‰
                raw_articles = crawler.get_naver_api_news(kw, display=10)
                if not raw_articles: continue

                full_contents = []
                main_image = None
                
                # 3-2. ë³¸ë¬¸ í¬ë¡¤ë§
                for art in raw_articles[:5]:
                    text, img = crawler.get_article_data(art['link'], target_keyword=kw)
                    
                    if text: full_contents.append(text)
                    if not main_image and img:
                        if img.startswith("http://"): img = img.replace("http://", "https://")
                        main_image = img

                # 3-3. (ë¹„ìƒìš©) ë³¸ë¬¸ ì‹¤íŒ¨ ì‹œ API Description ì‚¬ìš©
                if not full_contents:
                    for art in raw_articles[:5]:
                        if art.get('description'):
                            full_contents.append(art['description'])

                if not full_contents:
                    print(f"      â˜ï¸ '{kw}': ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ìŠ¤í‚µ")
                    continue

                # 3-4. AI ìš”ì•½
                briefing = ai_engine.synthesize_briefing(kw, full_contents)
                
                # í‰ì  ê³„ì‚° (ê¸°ë³¸ 7.0 ì´ìƒ)
                ai_score = round(9.9 - (rank * 0.1), 1)
                if ai_score < 7.0: ai_score = 7.0

                final_img = main_image or f"[https://placehold.co/600x400/111/cyan?text=](https://placehold.co/600x400/111/cyan?text=){kw}"

                news_item = {
                    "category": category,
                    "rank": rank + 1,
                    "keyword": kw,
                    "type": k_type, # íƒ€ì… ì •ë³´ ì €ì¥ (ë‚˜ì¤‘ì— í•„í„°ë§ìš©)
                    "title": f"[{kw}] News Update",
                    "summary": briefing,
                    "link": None, 
                    "image_url": final_img,
                    "score": ai_score,
                    "likes": 0, "dislikes": 0,
                    "created_at": datetime.now().isoformat(),
                    "published_at": datetime.now().isoformat()
                }
                category_news_list.append(news_item)
                time.sleep(0.5) 
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # [4ë‹¨ê³„] ì €ì¥ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ë°˜ì˜)
        if category_news_list:
            # 1. Live News: 1~30ìœ„ ì „ë¶€ ì €ì¥ (ì‚¬ëŒ í¬í•¨)
            repository.refresh_live_news(category, category_news_list)
            
            # 2. Trending Rankings: 'content' íƒ€ì…ì¸ ê²ƒë§Œ ê³¨ë¼ì„œ Top 10 ì €ì¥
            # (ì‚¬ëŒ ì´ë¦„ ì œì™¸, ê³¡ëª…/ì‘í’ˆëª…ë§Œ)
            content_only_list = [n for n in category_news_list if n.get('type') == 'content']
            
            # ë§Œì•½ content íƒ€ì…ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì–´ì©” ìˆ˜ ì—†ì´ ì„ì´ì§€ ì•Šë„ë¡, ìˆëŠ” ê²ƒë§Œì´ë¼ë„ ì €ì¥
            repository.update_sidebar_rankings(category, content_only_list[:10])
            
            # 3. Search Archive: í‰ì  7.0 ì´ìƒë§Œ ì €ì¥
            high_score_news = [n for n in category_news_list if n['score'] >= 7.0]
            if high_score_news:
                repository.save_to_archive(high_score_news)

    print("\nğŸ‰ ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

if __name__ == "__main__":
    run_master_scraper()

import sys
import os
import time
from datetime import datetime, timedelta
from dateutil import parser
from dotenv import load_dotenv

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì°¸ì¡° ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS

load_dotenv()

# ìœ ë£Œ ë²„ì „ì˜ í™”ë ¥ì„ í™œìš©í•´ ë¶„ì„ ë²”ìœ„ë¥¼ 30ìœ„ê¹Œì§€ í™•ëŒ€
TARGET_RANK_LIMIT = 30 

def is_within_24h(date_str):
    if not date_str: return False
    try:
        pub_date = parser.parse(date_str)
        if pub_date.tzinfo:
            pub_date = pub_date.replace(tzinfo=None)
        now = datetime.now()
        diff = now - pub_date
        return diff <= timedelta(hours=24)
    except:
        return False

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # [1ë‹¨ê³„] ì”¨ì•— ë°ì´í„° ìˆ˜ì§‘ (24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ ìš”ì•½ë³¸ë“¤)
        raw_text_data = [] 
        
        try:
            for seed in seeds:
                # [ìˆ˜ì •] 24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë¥¼ ë” ë§ì´ í™•ë³´í•˜ê¸° ìœ„í•´ displayë¥¼ 100ìœ¼ë¡œ ìƒí–¥
                news_items = crawler.get_naver_api_news(seed, display=100)
                for item in news_items:
                    if is_within_24h(item.get('pubDate')):
                        combined_text = f"Title: {item['title']}\nSummary: {item['description']}"
                        raw_text_data.append(combined_text)
            
            # AI ì…ë ¥ìš© ë°ì´í„° ì œí•œ
            raw_text_data = raw_text_data[:60]
            print(f"   ğŸŒ± 24ì‹œê°„ ë‚´ ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì§‘: {len(raw_text_data)}ê°œ")
            
            if len(raw_text_data) < 1:
                print("   âš ï¸ ê¸°ì‚¬ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
                
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            continue
        
        # [2ë‹¨ê³„] AI í‚¤ì›Œë“œ ì¶”ì¶œ
        top_entities = ai_engine.extract_top_entities(category, "\n".join(raw_text_data))
        
        if not top_entities: 
            print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ í˜¹ì€ ìœ íš¨í•œ í‚¤ì›Œë“œ ì—†ìŒ")
            continue
            
        print(f"   ğŸ’ ìœ íš¨ í‚¤ì›Œë“œ (Top 5): {', '.join([e['keyword'] for e in top_entities[:5]])}...")

        # [3ë‹¨ê³„] í‚¤ì›Œë“œë³„ ì‹¬ì¸µ ë¶„ì„ (30ìœ„ê¹Œì§€)
        category_news_list = []
        target_list = top_entities[:TARGET_RANK_LIMIT]
        
        for rank, entity in enumerate(target_list):
            kw = entity.get('keyword')
            k_type = entity.get('type', 'content')
            
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ({k_type}) ë¶„ì„ ì¤‘...")
            
            try:
                # [ìˆ˜ì •] íŠ¹ì • í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì—ë„ ê¸°ì‚¬ë¥¼ 100ê°œë¡œ ìƒí–¥
                raw_articles = crawler.get_naver_api_news(kw, display=100)
                if not raw_articles: continue

                full_contents = []
                main_image = None
                valid_article_count = 0
                
                for art in raw_articles:
                    if not is_within_24h(art.get('pubDate')): continue
                    
                    # [ë“¤ì—¬ì“°ê¸° ìˆ˜ì •] ì—ëŸ¬ê°€ ë‚¬ë˜ ë¶€ë¶„ì˜ ê³µë°±ì„ ì£¼ë³€ ì½”ë“œì™€ ë§ì·„ìŠµë‹ˆë‹¤.
                    text, img = crawler.get_article_data(art['link'])
                    
                    if text: 
                        full_contents.append(text)
                        valid_article_count += 1
                        if not main_image and img:
                            if img.startswith("http://"): 
                                img = img.replace("http://", "https://")
                            main_image = img
                            
                    if valid_article_count >= 30: 
                        break

                if not full_contents:
                    print(f"      â˜ï¸ '{kw}': ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨ (Skip)")
                    continue

                # [4ë‹¨ê³„] AI ë¸Œë¦¬í•‘ ë° ì œëª© ìƒì„±
                # [ìˆ˜ì •] ì´ì œ AIê°€ ì œëª©(title)ê³¼ ë‚´ìš©(summary)ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
                ai_data = ai_engine.synthesize_briefing(kw, full_contents)
                
                if not ai_data or not ai_data.get('summary'):
                    print(f"      ğŸ—‘ï¸ '{kw}': ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨ë¡œ íê¸°")
                    continue
                
                ai_score = round(9.9 - (rank * 0.1), 1)
                if ai_score < 7.0: ai_score = 7.0

                final_img = main_image or f"https://placehold.co/600x400/111/cyan?text={kw}"

                news_item = {
                    "category": category,
                    "rank": rank + 1,
                    "keyword": kw,
                    "type": k_type,
                    # [ìˆ˜ì •] ê³ ì •ëœ ì œëª© ëŒ€ì‹  AIê°€ ìƒì„±í•œ ì œëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                    "title": ai_data.get('title', f"[{kw}] Special Report"),
                    "summary": ai_data.get('summary'),
                    "link": None,
                    "image_url": final_img,
                    "score": ai_score,
                    "likes": 0, "dislikes": 0,
                    "created_at": datetime.now().isoformat(),
                    "published_at": datetime.now().isoformat()
                }
                category_news_list.append(news_item)
                
                time.sleep(0.5) 
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue

        # [5ë‹¨ê³„] ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì‚° ì €ì¥
        if category_news_list:
            print(f"   ğŸ’¾ ì €ì¥ ì‹œì‘: ì´ {len(category_news_list)}ê°œ")
            repository.refresh_live_news(category, category_news_list)
            
            content_only_list = [n for n in category_news_list if n.get('type') == 'content']
            final_ranking_list = []
            source_list = content_only_list if len(content_only_list) >= 3 else category_news_list

            for new_rank, item in enumerate(source_list[:10]):
                ranked_item = item.copy()
                ranked_item['rank'] = new_rank + 1
                final_ranking_list.append(ranked_item)
                
            repository.update_sidebar_rankings(category, final_ranking_list)
            repository.save_to_archive(category_news_list)

    print("\nğŸ‰ ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

if __name__ == "__main__":
    run_master_scraper()

import os
import sys
import json
import time
import random
import requests
from supabase import create_client, Client
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq

load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase: Client = create_client(os.environ.get("NEXT_PUBLIC_SUPABASE_URL"), os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY"))
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

# [ìš”êµ¬ì‚¬í•­ 2] ìµœì‹  ëª¨ë¸ë¶€í„° ì°¨ë¡€ë¡œ ì‹œë„
MODELS_TO_TRY = ["llama-3.3-70b-versatile", "llama-3.1-70b-versatile", "llama-3.1-8b-instant"]

# [ìš”êµ¬ì‚¬í•­ 1] ë³´ì™„ ì „ëµ í‚¤ì›Œë“œ ì „ì²´ ë°˜ì˜
SEARCH_KEYWORDS = [
    "ì»´ë°± ì´ˆë™ ì‹ ê¸°ë¡", "ì•„ì´ëŒ ë¹Œë³´ë“œ ë…ì ", "ë®¤ì§ë¹„ë””ì˜¤ 1ì–µë·°", "ì±Œë¦°ì§€ ìœ í–‰", "ì— ì¹´ 1ìœ„", "ì•„ì´ëŒ í¬í† ì¹´ë“œ",
    "ë“œë¼ë§ˆ ìºìŠ¤íŒ… í™•ì •", "OTT ìˆœìœ„ 1ìœ„", "ë“œë¼ë§ˆ ì œì‘ë°œí‘œíšŒ", "ë“œë¼ë§ˆ ë°˜ì „ ê²°ë§", "ì¸ìƒ ìºë¦­í„° ë°°ìš°",
    "ì²œë§Œ ì˜í™” ê´€ê°ìˆ˜", "ì˜í™”ì œ ìˆ˜ìƒ ë…ì ", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ì‹¤ì‹œê°„ ì˜ˆë§¤ìœ¨", "ì˜í™” ì‹œì‚¬íšŒ ë¬´ëŒ€ì¸ì‚¬",
    "ì˜ˆëŠ¥ ëŒ€ìƒ í›„ë³´", "ì›¹ì˜ˆëŠ¥ ìœ íŠœë¸Œ í™”ì œ", "ì˜ˆëŠ¥ ì‹œì²­ë¥  ëŒ€ë°•", "ì˜ˆëŠ¥ ë² ìŠ¤íŠ¸ ì»¤í”Œ",
    "K-í‘¸ë“œ í•´ì™¸ ë°˜ì‘", "K-ë·°í‹° ì‹ ìƒ", "ì„±ìˆ˜ë™ íŒì—…ìŠ¤í† ì–´", "ì¸ê¸° ì›¹íˆ° ë“œë¼ë§ˆí™”", "K-íŒ¨ì…˜ ê¸€ë¡œë²Œ ì „ì‹œ"
]

def get_naver_api_news(keyword):
    import urllib.parse, urllib.request
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=20&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

def ai_chief_editor(news_batch):
    raw_text = "\n".join([f"[{i}] {n['title']}" for i, n in enumerate(news_batch)])
    prompt = f"""
    Task: Analyze these {len(news_batch)} news items. 
    1. Select exactly 30 news items and rank them 1 to 30 based on buzzworthiness.
    2. Categorize into [k-pop, k-drama, k-movie, k-entertain, k-culture].
    3. Generate a ONE-SENTENCE "Global Insight" based on REAL top news.
    
    News: {raw_text}
    
    Output JSON:
    {{
        "global_insight": "Actual trend summary...",
        "articles": [
            {{ "original_index": 0, "rank": 1, "category": "k-pop", "eng_title": "...", "summary": "3-line summary", "score": 9.5 }}
        ]
    }}
    """
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(messages=[{"role": "user", "content": prompt}], model=model, response_format={"type": "json_object"})
            return json.loads(res.choices[0].message.content)
        except: continue
    return None

def run():
    print("ğŸš€ ë‰´ìŠ¤ ì—”ì§„ ê°€ë™...")
    all_news = []
    for kw in SEARCH_KEYWORDS:
        all_news.extend(get_naver_api_news(kw))
    
    result = ai_chief_editor(all_news)
    if not result: return

    # [ìš”êµ¬ì‚¬í•­ 4] ì¸ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸ (ë³„ë„ í…Œì´ë¸” í˜¹ì€ ì²« ë²ˆì§¸ ê¸°ì‚¬ì— ì €ì¥)
    global_insight = result.get('global_insight', "K-Enter news is trending worldwide.")
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (Fresh Start)
    supabase.table("live_news").delete().neq("id", "0000").execute()

    for art in result.get('articles', []):
        orig = all_news[art['original_index']]
        data = {
            "rank": art['rank'],
            "category": art['category'],
            "title": art['eng_title'],
            "summary": art['summary'],
            "link": orig['link'],
            "score": art['score'],
            "insight": global_insight, # ëª¨ë“  ê¸°ì‚¬ê°€ ìµœì‹  ì¸ì‚¬ì´íŠ¸ë¥¼ ê³µìœ í•˜ê²Œ ì €ì¥
            "likes": 0, "dislikes": 0,
            "created_at": datetime.now().isoformat()
        }
        supabase.table("live_news").insert(data).execute()
    print(f"âœ… {len(result['articles'])}ê°œ ë‰´ìŠ¤ ë­í‚¹ ì™„ë£Œ.")

if __name__ == "__main__":
    run()

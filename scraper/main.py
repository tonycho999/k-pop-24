import os
import json
import time
import requests
from supabase import create_client, Client
from dotenv import load_dotenv
from ddgs import DDGS

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. Supabase ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. Gemini API í‚¤ ì„¤ì •
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

# API í‚¤ í™•ì¸
if GOOGLE_API_KEY:
    print(f"ğŸ”‘ API Key ë¡œë“œ ì™„ë£Œ: {GOOGLE_API_KEY[:5]}...")
else:
    print("âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤!")

CATEGORIES = {
    "K-Pop": "k-pop latest news trends",
    "K-Drama": "k-drama ratings news",
    "K-Movie": "korean movie box office news",
    "K-Variety": "korean variety show news",
    "K-Culture": "seoul travel food trends"
}

# [í•µì‹¬] ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ ì°¾ëŠ” í•¨ìˆ˜
def get_dynamic_model_url():
    print("ğŸ” êµ¬ê¸€ ì„œë²„ì— ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
    list_url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GOOGLE_API_KEY}"
    
    try:
        response = requests.get(list_url)
        if response.status_code != 200:
            print(f"âš ï¸ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ({response.status_code}): ê¸°ë³¸ê°’ ì‚¬ìš©")
            return "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
            
        data = response.json()
        models = data.get('models', [])
        
        # 'flash'ê°€ í¬í•¨ë˜ê³  'generateContent' ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ ì°¾ê¸°
        valid_models = []
        for m in models:
            name = m['name'] # ì˜ˆ: models/gemini-1.5-flash
            methods = m.get('supportedGenerationMethods', [])
            if 'generateContent' in methods and 'flash' in name:
                valid_models.append(name)
        
        if valid_models:
            # ê°€ì¥ ìµœì‹  ëª¨ë¸ ì„ íƒ (ë³´í†µ ë¦¬ìŠ¤íŠ¸ ë’¤ìª½ì´ ìµœì‹ )
            best_model = valid_models[-1]
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì  ëª¨ë¸ ë°œê²¬: {best_model}")
            # models/gemini-1.5-flash -> https://.../models/gemini-1.5-flash:generateContent
            return f"https://generativelanguage.googleapis.com/v1beta/{best_model}:generateContent"
        
        print("âš ï¸ Flash ëª¨ë¸ì„ ì°¾ì§€ ëª»í•¨. ê¸°ë³¸ ëª¨ë¸(gemini-pro) ì‹œë„.")
        return "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent"

    except Exception as e:
        print(f"âŒ ëª¨ë¸ íƒìƒ‰ ì¤‘ ì—ëŸ¬: {e}")
        return "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ URL í™•ì •
CURRENT_MODEL_URL = get_dynamic_model_url()

def search_web(keyword):
    """DuckDuckGo ê²€ìƒ‰"""
    print(f"ğŸ” [Search] '{keyword}' ê²€ìƒ‰ ì¤‘...")
    results = []
    try:
        with DDGS() as ddgs:
            # 1. ë‰´ìŠ¤ ê²€ìƒ‰
            ddg_results = list(ddgs.news(query=keyword, region="kr-kr", safesearch="off", max_results=10))
            
            # 2. í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ë°±ì—…)
            if not ddg_results:
                time.sleep(1)
                ddg_results = list(ddgs.text(query=keyword, region="kr-kr", max_results=5))

            for r in ddg_results:
                title = r.get('title', '')
                body = r.get('body', r.get('snippet', ''))
                link = r.get('url', r.get('href', ''))
                if title and body:
                    results.append(f"ì œëª©: {title}\në‚´ìš©: {body}\në§í¬: {link}")
                
    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆëœ€): {e}")
    
    return "\n\n".join(results)

def call_gemini_api(category_name, raw_data):
    print(f"ğŸ¤– [Gemini] '{category_name}' ë¶„ì„ ìš”ì²­ ì¤‘...")
    
    headers = {"Content-Type": "application/json"}
    
    prompt = f"""
    You are a K-Entertainment news editor.
    Raw data: {raw_data[:15000]} 

    Task: Extract 10 news items and Top 10 rankings.
    Output must be strict JSON without Markdown.

    Format:
    {{
      "news_updates": [
        {{ "keyword": "Subject", "title": "Title", "summary": "Summary", "link": "URL" }}
      ],
      "rankings": [
        {{ "rank": 1, "title": "Name", "meta": "Info" }}
      ]
    }}
    """
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        full_url = f"{CURRENT_MODEL_URL}?key={GOOGLE_API_KEY}"
        response = requests.post(full_url, headers=headers, json=payload)
        
        if response.status_code == 200:
            try:
                text = response.json()['candidates'][0]['content']['parts'][0]['text']
                text = text.replace("```json", "").replace("```", "").strip()
                return json.loads(text)
            except Exception as e:
                print(f"   âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return None
        else:
            print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨ ({response.status_code}): {response.text[:200]}")
            return None
            
    except Exception as e:
        print(f"   âŒ ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def update_database(category, data):
    news_list = data.get("news_updates", [])
    if news_list:
        clean_news = []
        for item in news_list:
            clean_news.append({
                "category": category,
                "keyword": item.get("keyword", category),
                "title": item.get("title", "ì œëª© ì—†ìŒ"),
                "summary": item.get("summary", ""),
                "link": item.get("link", ""),
                "created_at": "now()"
            })
        
        try:
            supabase.table("live_news").upsert(clean_news, on_conflict="category,keyword,title").execute()
            supabase.table("search_archive").upsert(clean_news, on_conflict="category,keyword,title").execute()
            print(f"   ğŸ’¾ ë‰´ìŠ¤ {len(clean_news)}ê°œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    rank_list = data.get("rankings", [])
    if rank_list:
        clean_ranks = []
        for item in rank_list:
            clean_ranks.append({
                "category": category,
                "rank": item.get("rank"),
                "title": item.get("title"),
                "meta_info": item.get("meta", ""),
                "updated_at": "now()"
            })
        try:
            supabase.table("live_rankings").upsert(clean_ranks, on_conflict="category,rank").execute()
            print(f"   ğŸ† ë­í‚¹ ê°±ì‹  ì™„ë£Œ")
        except Exception:
            pass

def main():
    print(f"ğŸš€ ìŠ¤í¬ë˜í¼ ì‹œì‘ (Model: {CURRENT_MODEL_URL.split('/')[-1]})")
    
    for category, search_keyword in CATEGORIES.items():
        raw_text = search_web(search_keyword)
        
        if len(raw_text) < 10: 
            print(f"âš ï¸ {category} ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€")
            continue

        data = call_gemini_api(category, raw_text)
        
        if data:
            update_database(category, data)
        
        time.sleep(3)

    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()

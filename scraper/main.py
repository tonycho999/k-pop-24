import json
import re
import os
import time
from news_api import NewsEngine
from naver_api import NaverManager
from database import DatabaseManager
from supabase import create_client

# ---------------------------------------------------------
# [ì„¤ì •] ì¹´ìš´í„° ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§ (0 ~ 23 ì‚¬ì´í´)
# ---------------------------------------------------------
# K-Pop: ë§¤ë²ˆ ì‹¤í–‰ (ì¡°ê±´ ì—†ìŒ)
# ê·¸ ì™¸ ì¹´í…Œê³ ë¦¬: ì•„ë˜ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” 'ìˆœì„œ'ì—ë§Œ ì‹¤í–‰
TARGET_COUNTS_FOR_OTHERS = [5, 17] 

def clean_json_text(text):
    match = re.search(r"```(?:json)?\s*(.*)\s*```", text, re.DOTALL)
    if match: text = match.group(1)
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1: return text[start:end+1]
    return text.strip()

# ---------------------------------------------------------
# [DB ì—°ë™] ì‹¤í–‰ ì¹´ìš´íŠ¸ ê´€ë¦¬ í•¨ìˆ˜
# ---------------------------------------------------------
supa_url = os.environ.get("SUPABASE_URL")
supa_key = os.environ.get("SUPABASE_KEY")
supabase = create_client(supa_url, supa_key)

def get_run_count():
    """DBì—ì„œ í˜„ì¬ run_count ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ 0)"""
    try:
        # [ìˆ˜ì •] .from() -> .table()
        res = supabase.table('system_status').select('run_count').eq('id', 1).single().execute()
        
        # supabase-py ìµœì‹  ë²„ì „ì—ì„œëŠ” .execute() ê²°ê³¼ë¥¼ ë°”ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ .dataë¡œ ì ‘ê·¼
        if res.data:
            return res.data['run_count']
        return 0
    except Exception as e:
        # í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 0 ë¦¬í„´
        print(f"âš ï¸ Init Run Count Error (Using 0): {e}")
        return 0

def update_run_count(current):
    """
    ì‹¤í–‰ì´ ëë‚˜ë©´ ì¹´ìš´íŠ¸ë¥¼ 1 ì˜¬ë¦¼
    """
    next_count = current + 1
    if next_count >= 24:
        next_count = 0
    
    try:
        # [ìˆ˜ì •] .from() -> .table()
        supabase.table('system_status').upsert({'id': 1, 'run_count': next_count}).execute()
        print(f"ğŸ”„ Cycle Count Updated: {current} -> {next_count}")
    except Exception as e:
        print(f"âš ï¸ Failed to update run count: {e}")

def is_target_run(category, run_count):
    """ì‹¤í–‰ ì—¬ë¶€ ê²°ì •"""
    # 1. K-POP: ë¬´ì¡°ê±´ ì‹¤í–‰ (ê°€ì¥ ì¤‘ìš”)
    if category == 'k-pop':
        return True
        
    # 2. ë‚˜ë¨¸ì§€: ì§€ì •ëœ ìˆœì„œ(5, 17)ì¼ ë•Œë§Œ ì‹¤í–‰
    if run_count in TARGET_COUNTS_FOR_OTHERS:
        return True
        
    print(f"  â­ï¸ [Skip] {category} (Current Count: {run_count})")
    return False

# ---------------------------------------------------------
# [ë©”ì¸ ë¡œì§]
# ---------------------------------------------------------
def run_automation():
    # 1. DBì—ì„œ 'ì´ë²ˆì—” ëª‡ ë²ˆì§¸ ìˆœì„œì¸ì§€' í™•ì¸
    run_count = get_run_count()
    print(f"ğŸš€ Automation Started (Cycle: {run_count}/23)")
    
    db = DatabaseManager()
    engine = NewsEngine()
    naver = NaverManager()
    
    categories = ["k-pop", "k-drama", "k-movie", "k-entertain", "k-culture"]

    for cat in categories:
        # ì‹¤í–‰í•  ìˆœì„œê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
        if not is_target_run(cat, run_count):
            continue
            
        print(f"\n[{cat}] Processing...")

        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘ (í•œêµ­ì–´ ê²€ìƒ‰ -> ì˜ì–´ JSON)
            raw_data_str, original_query = engine.get_trends_and_rankings(cat)
            
            cleaned_str = clean_json_text(raw_data_str)
            if not cleaned_str or cleaned_str == "{}":
                print(f"âš ï¸ [{cat}] No data returned.")
                continue

            parsed_data = json.loads(cleaned_str)
            
            # A. ë­í‚¹ ì €ì¥
            top10_list = parsed_data.get('top10', [])
            if top10_list:
                print(f"  > Saving {len(top10_list)} Rankings...")
                for item in top10_list:
                    db.save_rankings([{
                        "category": cat,
                        "rank": item.get('rank'),
                        "title": item.get('title'),
                        "meta_info": item.get('info', ''),
                        "score": 0
                    }])

            # B. ê¸°ì‚¬ ì‘ì„±
            people_list = parsed_data.get('people', [])
            if people_list:
                print(f"  > Processing {len(people_list)} Articles...")
                
                for person in people_list:
                    name_en = person.get('name_en')
                    name_kr = person.get('name_kr')
                    facts = person.get('facts')
                    
                    if not name_en: name_en = name_kr 
                    if not name_kr: name_kr = name_en
                    
                    if not name_en: continue

                    # Groq ê¸°ì‚¬ ìƒì„±
                    full_text = engine.edit_with_groq(name_en, facts, cat)
                    
                    # ì ìˆ˜ íŒŒì‹±
                    score = 70
                    final_text = full_text
                    if "###SCORE:" in full_text:
                        try:
                            parts = full_text.split("###SCORE:")
                            final_text = parts[0].strip()
                            import re
                            score_match = re.search(r'\d+', parts[1])
                            if score_match: score = int(score_match.group())
                        except: pass

                    lines = final_text.split('\n')
                    title = lines[0].replace('Headline:', '').strip()
                    summary = "\n".join(lines[1:]).strip()
                    
                    # ì´ë¯¸ì§€ ê²€ìƒ‰
                    img_url = naver.get_image(name_kr)
                    
                    article_data = {
                        "category": cat,
                        "keyword": name_en,
                        "title": title,
                        "summary": summary,
                        "link": person.get('link', ''),
                        "image_url": img_url,
                        "score": score,
                        "likes": 0,
                        "query": original_query,
                        "raw_result": str(person),
                        "run_count": run_count 
                    }
                    
                    db.save_to_archive(article_data)
                    
                    live_data = {
                        "category": article_data['category'],
                        "keyword": article_data['keyword'],
                        "title": article_data['title'],
                        "summary": article_data['summary'],
                        "link": article_data['link'],
                        "image_url": article_data['image_url'],
                        "score": score,
                        "likes": 0
                    }
                    db.save_live_news([live_data])
                    print(f"    - Published: {name_en} (Score: {score})")

        except Exception as e:
            print(f"âŒ [{cat}] Error: {e}")

    # 2. ëª¨ë“  ì‘ì—…ì´ ëë‚˜ë©´ ë‹¤ìŒ ìˆœì„œë¥¼ ìœ„í•´ ì¹´ìš´íŠ¸ +1
    update_run_count(run_count)

if __name__ == "__main__":
    run_automation()

import os
import sys
import json
import time
import requests
from supabase import create_client, Client
from datetime import datetime, timedelta
from dotenv import load_dotenv
from groq import Groq

load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase: Client = create_client(os.environ.get("NEXT_PUBLIC_SUPABASE_URL"), os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY"))
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

MODELS_TO_TRY = ["llama-3.3-70b-versatile", "llama-3.1-70b-versatile"]

# [ìˆ˜ì •] ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤í•‘ (ë¶„í•  ìˆ˜ì§‘ì„ ìœ„í•´)
CATEGORY_MAP = {
    "k-pop": ["ì»´ë°±", "ë¹Œë³´ë“œ", "ì•„ì´ëŒ", "ë®¤ì§", "ë¹„ë””ì˜¤", "ì±Œë¦°ì§€", "í¬í† ì¹´ë“œ", "ì›”ë“œíˆ¬ì–´", "ê°€ìˆ˜"],
    "k-drama": ["ë“œë¼ë§ˆ", "ì‹œì²­ë¥ ", "ë„·í”Œë¦­ìŠ¤", "OTT", "ë°°ìš°", "ìºìŠ¤íŒ…", "ëŒ€ë³¸ë¦¬ë”©", "ì¢…ì˜"],
    "k-movie": ["ì˜í™”", "ê°œë´‰", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "ì‹œì‚¬íšŒ", "ì˜í™”ì œ", "ê´€ê°", "ë¬´ëŒ€ì¸ì‚¬", "ê°œë´‰"],
    "k-entertain": ["ì˜ˆëŠ¥", "ìœ íŠœë¸Œ", "ê°œê·¸ë§¨", "ì½”ë¯¸ë””ì–¸", "ë°©ì†¡", "ê°œê·¸ìš°ë¨¼"],
    "k-culture": ["í‘¸ë“œ", "ë·°í‹°", "ì›¹íˆ°", "íŒì—…ìŠ¤í† ì–´", "íŒ¨ì…˜", "ìŒì‹", "í•´ì™¸ë°˜ì‘"]
}

def get_naver_api_news(keyword):
    import urllib.parse, urllib.request
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

def get_article_image(link):
    from bs4 import BeautifulSoup
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get(link, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        og_image = soup.find('meta', property='og:image')
        return og_image['content'] if og_image else None
    except: return None

def ai_category_editor(category, news_batch):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— íŠ¹í™”í•˜ì—¬ 30ê°œë¥¼ ë°˜ë“œì‹œ ì„ ë³„í•˜ë„ë¡ ìš”ì²­"""
    # ë„ˆë¬´ ë§ì€ ì…ë ¥ì€ AIê°€ í˜¼ë€ìŠ¤ëŸ¬ì›Œí•˜ë¯€ë¡œ ìƒìœ„ 150ê°œ ì •ë„ë¡œ ì œí•œ
    limited_batch = news_batch[:150]
    raw_text = "\n".join([f"[{i}] {n['title']}" for i, n in enumerate(limited_batch)])
    
    prompt = f"""
    Task: Select the TOP 30 most buzzworthy news items for the '{category}' category from the list below.
    
    Constraints:
    1. You MUST select EXACTLY 30 items.
    2. Rank them from 1 to 30.
    3. Translate titles to English and write a 3-line English summary for each.
    4. Provide an AI Score (0.0 to 10.0) based on trend potential.

    List:
    {raw_text}

    Output JSON Format:
    {{
        "articles": [
            {{ "original_index": 0, "rank": 1, "category": "{category}", "eng_title": "...", "summary": "...", "score": 9.5 }}
        ]
    }}
    """
    
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(
                messages=[{"role": "system", "content": "You are a professional K-Enter Editor."},
                          {"role": "user", "content": prompt}], 
                model=model, 
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content).get('articles', [])
        except: continue
    return []

def run():
    print("ğŸš€ ë‰´ìŠ¤ ì—”ì§„ ê°€ë™ (ë¶„í•  ì²˜ë¦¬ ëª¨ë“œ)...")

    # 1. 24ì‹œê°„ ì§€ë‚œ ë‰´ìŠ¤ ì‚­ì œ
    time_threshold = (datetime.now() - timedelta(hours=24)).isoformat()
    supabase.table("live_news").delete().lt("created_at", time_threshold).execute()

    # 2. ê¸°ì¡´ live_news ë°±ì—… (ì¢‹ì•„ìš” ìƒìœ„ 10ê°œ)
    try:
        top_voted = supabase.table("live_news").select("*").order("likes", desc=True).limit(10).execute()
        for item in top_voted.data:
            archive_data = {
                "original_link": item['link'], "category": item['category'], "title": item['title'],
                "summary": item['summary'], "image_url": item['image_url'], "score": item['score'], "archive_reason": "Top 10 Likes"
            }
            supabase.table("search_archive").upsert(archive_data, on_conflict="original_link").execute()
    except: pass

    # 3. ì‹ ê·œ ì‹¤ì‹œê°„ ë­í‚¹ ë°ì´í„° ì‚­ì œ (ì´ˆê¸°í™”)
    supabase.table("live_news").delete().neq("id", "00000000-0000-0000-0000-000000000000").execute()

    final_articles = []
    
    # [í•µì‹¬] ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëŒë©´ì„œ ìˆ˜ì§‘ ë° AI ë¶„ì„
    for category, keywords in CATEGORY_MAP.items():
        print(f"ğŸ“‚ {category.upper()} ë¶€ë¬¸ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘...")
        cat_news = []
        for kw in keywords:
            cat_news.extend(get_naver_api_news(kw))
        
        # ì¤‘ë³µ ì œê±°
        cat_news = list({n['link']: n for n in cat_news}.values())
        
        # AIì—ê²Œ ì´ ì¹´í…Œê³ ë¦¬ì—ì„œ 30ê°œ ë½‘ìœ¼ë¼ê³  ëª…ë ¹
        selected = ai_category_editor(category, cat_news)
        print(f"   ã„´ AI ì„ ë³„ ì™„ë£Œ: {len(selected)}ê°œ")
        
        # ì‹¤ì œ ë°ì´í„° ë§¤ì¹­ ë° ì €ì¥ ì¤€ë¹„
        for art in selected:
            idx = art['original_index']
            if idx >= len(cat_news): continue
            
            orig = cat_news[idx]
            img = get_article_image(orig['link'])
            if not img: img = f"https://placehold.co/600x400/111/cyan?text={category}"

            data = {
                "rank": art['rank'], # ì¹´í…Œê³ ë¦¬ ë‚´ ìˆœìœ„
                "category": category,
                "title": art['eng_title'],
                "summary": art['summary'],
                "link": orig['link'],
                "image_url": img,
                "score": art['score'],
                "likes": 0, "dislikes": 0,
                "created_at": datetime.now().isoformat()
            }
            
            # DB ì €ì¥
            supabase.table("live_news").insert(data).execute()
            
            # ì•„ì¹´ì´ë¸Œ (ì¹´í…Œê³ ë¦¬ 1~3ìœ„ëŠ” ë¬´ì¡°ê±´ ì €ì¥)
            if art['rank'] <= 3:
                archive_data = {
                    "original_link": orig['link'], "category": category, "title": art['eng_title'],
                    "summary": art['summary'], "image_url": img, "score": art['score'], "archive_reason": f"{category} Top 3"
                }
                supabase.table("search_archive").upsert(archive_data, on_conflict="original_link").execute()
            
            final_articles.append(data)

    print(f"ğŸ‰ ìµœì¢… ì™„ë£Œ: ì´ {len(final_articles)}ê°œì˜ ë‰´ìŠ¤ê°€ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run()

import sys
import os
import time
from datetime import datetime
from dotenv import load_dotenv

# ëª¨ë“ˆ import ë¬¸ì œ ë°©ì§€
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

# í•„ìˆ˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS, TOP_RANK_LIMIT

load_dotenv()

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    # 5ê°œ ì¹´í…Œê³ ë¦¬ ë£¨í”„
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # [1ë‹¨ê³„] ì”¨ì•— ìˆ˜ì§‘ (Seed Search)
        # ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ í†µí•´ ê´‘ë²”ìœ„í•œ ì œëª© ìˆ˜ì§‘ (ì°¨ë‹¨ ë°©ì§€ & ìµœì‹ ì„± í™•ë³´)
        seed_titles = []
        try:
            for seed in seeds:
                # ê° ì‹œë“œë‹¹ 20~30ê°œ ì •ë„ë§Œ ê°€ì ¸ì™€ì„œ ë¯¹ìŠ¤
                news = crawler.get_naver_api_news(seed, display=20)
                seed_titles.extend([n['title'] for n in news])
            
            # ì¤‘ë³µ ì œê±°
            seed_titles = list(set(seed_titles))
            print(f"   ğŸŒ± ì›ì„ ìˆ˜ì§‘ ì™„ë£Œ: {len(seed_titles)}ê°œì˜ ì œëª© í™•ë³´")
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
        
        # [2ë‹¨ê³„] ì—”í‹°í‹° ì¶”ì¶œ ë° ë­í‚¹ (AI Mining)
        top_keywords = ai_engine.extract_top_entities(category, seed_titles)
        
        if not top_keywords:
            print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™.")
            continue
            
        print(f"   ğŸ’ ì¶”ì¶œëœ ë­í‚¹(Top {len(top_keywords)}): {', '.join(top_keywords[:5])}...")

        # [3ë‹¨ê³„] ì •ë°€ ê²€ìƒ‰ ë° í•©ì„± (Deep Dive & Synthesis)
        category_news_list = []
        
        # ìƒìœ„ Nê°œ(ì„¤ì •ê°’ 30ê°œ)ë§Œ ì²˜ë¦¬
        target_keywords = top_keywords[:TOP_RANK_LIMIT]
        
        for rank, kw in enumerate(target_keywords):
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ë¶„ì„ ì¤‘...")
            
            try:
                # í•´ë‹¹ í‚¤ì›Œë“œë¡œ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰
                raw_articles = crawler.get_naver_api_news(kw, display=10)
                
                if not raw_articles:
                    continue

                # ë³¸ë¬¸ í¬ë¡¤ë§ (ìƒìœ„ 3~5ê°œ ê¸°ì‚¬ í•©ì¹˜ê¸°)
                full_contents = []
                main_image = None
                valid_link = raw_articles[0]['link']
                published_at = raw_articles[0].get('published_at', datetime.now()).isoformat()

                for art in raw_articles[:5]:
                    text, img = crawler.get_article_data(art['link'])
                    if text: full_contents.append(text)
                    # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ë©”ì¸ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©
                    if not main_image and img: main_image = img

                # AI ìš”ì•½ (ë¸Œë¦¬í•‘ ìƒì„±)
                if full_contents:
                    briefing = ai_engine.synthesize_briefing(kw, full_contents)
                    
                    # ì´ë¯¸ì§€ ì—†ì„ ê²½ìš° í”Œë ˆì´ìŠ¤í™€ë”
                    final_img = main_image or f"https://placehold.co/600x400/111/cyan?text={kw}"

                    news_item = {
                        "category": category,
                        "rank": rank + 1,       # ë­í‚¹ ì •ë³´ ì¶”ê°€
                        "keyword": kw,          # í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
                        "title": f"[{rank+1}] {kw}: Top Trending News", # ì œëª© í¬ë§·íŒ…
                        "summary": briefing,
                        "link": valid_link,     # ëŒ€í‘œ ë§í¬ í•˜ë‚˜ ì œê³µ
                        "image_url": final_img,
                        "score": 10.0 - (rank * 0.1), # ë­í‚¹ ê¸°ë°˜ ì ìˆ˜ (1ìœ„ 10ì , 2ìœ„ 9.9ì ...)
                        "likes": 0, "dislikes": 0,
                        "created_at": datetime.now().isoformat(),
                        "published_at": published_at
                    }
                    category_news_list.append(news_item)
                
                # API ë³´í˜¸ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.5)
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # [4ë‹¨ê³„] DB ì €ì¥ (êµì²´ ë°©ì‹)
        if category_news_list:
            # 1. ìƒìœ„ 10ê°œ ì•„ì¹´ì´ë¸Œ ì €ì¥
            repository.save_to_archive(category_news_list[:10])
            
            # 2. Live News í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì „ì²´ êµì²´
            repository.refresh_live_news(category, category_news_list)

    print("\nğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ 150ê°œ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_master_scraper()

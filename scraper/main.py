import os
import json
import time
import requests
from supabase import create_client, Client
from dotenv import load_dotenv
from ddgs import DDGS

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 2. Supabase ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. Gemini API í‚¤ ì„¤ì •
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

# [ì„¤ì •] ê²€ìƒ‰ì–´ ìµœì í™”
CATEGORIES = {
    "K-Pop": "k-pop latest news trends",
    "K-Drama": "k-drama ratings news",
    "K-Movie": "korean movie box office news",
    "K-Variety": "korean variety show news",
    "K-Culture": "seoul travel food trends"
}

def search_web(keyword):
    """DuckDuckGo ê²€ìƒ‰ (íŒŒë¼ë¯¸í„°ëª… queryë¡œ ìˆ˜ì •ë¨)"""
    print(f"ğŸ” [Search] '{keyword}' ê²€ìƒ‰ ì¤‘...")
    results = []
    try:
        with DDGS() as ddgs:
            # [ìˆ˜ì • 1] keywords -> query ë¡œ ë³€ê²½ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸ ëŒ€ì‘)
            ddg_results = list(ddgs.news(query=keyword, region="kr-kr", safesearch="off", max_results=10))
            
            # 2. ë‰´ìŠ¤ ì—†ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œë„
            if not ddg_results:
                time.sleep(1)
                # [ìˆ˜ì • 2] text ê²€ìƒ‰ë„ queryë¡œ ë³€ê²½
                ddg_results = list(ddgs.text(query=keyword, region="kr-kr", max_results=5))

            for r in ddg_results:
                title = r.get('title', '')
                body = r.get('body', r.get('snippet', ''))
                link = r.get('url', r.get('href', ''))
                if title and body:
                    results.append(f"ì œëª©: {title}\në‚´ìš©: {body}\në§í¬: {link}")
                
    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
    
    return "\n\n".join(results)

def call_gemini_api(category_name, raw_data):
    """
    [í•µì‹¬] ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ëŠ” 'ìƒì¡´í˜•' API í˜¸ì¶œ í•¨ìˆ˜
    """
    print(f"ğŸ¤– [Gemini] '{category_name}' ë¶„ì„ ìš”ì²­ ì¤‘...")
    
    # ì‹œë„í•  ëª¨ë¸ í›„ë³´êµ° (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
    endpoints = [
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent",
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent",
    ]
    
    headers = {"Content-Type": "application/json"}
    
    prompt = f"""
    You are a K-Entertainment news editor.
    Raw data: {raw_data[:15000]} 

    Task: Extract 10 news items and Top 10 rankings.
    Output must be strict JSON.

    Format:
    {{
      "news_updates": [
        {{ "keyword": "Subject", "title": "Title", "summary": "Summary", "link": "URL" }}
      ],
      "rankings": [
        {{ "rank": 1, "title": "Name", "meta": "Info" }}
      ]
    }}
    """
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    
    for url in endpoints:
        try:
            full_url = f"{url}?key={GOOGLE_API_KEY}"
            response = requests.post(full_url, headers=headers, json=payload)
            
            if response.status_code == 200:
                print(f"   âœ… ì„±ê³µ! (ì‚¬ìš©ëœ ëª¨ë¸: {url.split('models/')[1].split(':')[0]})")
                try:
                    text = response.json()['candidates'][0]['content']['parts'][0]['text']
                    text = text.replace("```json", "").replace("```", "").strip()
                    return json.loads(text)
                except Exception:
                    continue 
            else:
                print(f"   âš ï¸ ì‹¤íŒ¨ ({response.status_code}): ë‹¤ìŒ ëª¨ë¸ ì‹œë„...")
                continue
                
        except Exception as e:
            print(f"   âŒ ì—°ê²° ì˜¤ë¥˜: {e}")
            continue

    print("âŒ ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨")
    return None

def update_database(category, data):
    # 1. ë‰´ìŠ¤ ì €ì¥
    news_list = data.get("news_updates", [])
    if news_list:
        clean_news = []
        for item in news_list:
            clean_news.append({
                "category": category,
                "keyword": item.get("keyword", category),
                "title": item.get("title", "ì œëª© ì—†ìŒ"),
                "summary": item.get("summary", ""),
                "link": item.get("link", ""),
                "created_at": "now()"
            })
        
        try:
            supabase.table("live_news").upsert(clean_news, on_conflict="category,keyword,title").execute()
            supabase.table("search_archive").upsert(clean_news, on_conflict="category,keyword,title").execute()
            print(f"   ğŸ’¾ ë‰´ìŠ¤ {len(clean_news)}ê°œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    # 2. ë­í‚¹ ì €ì¥
    rank_list = data.get("rankings", [])
    if rank_list:
        clean_ranks = []
        for item in rank_list:
            clean_ranks.append({
                "category": category,
                "rank": item.get("rank"),
                "title": item.get("title"),
                "meta_info": item.get("meta", ""),
                "updated_at": "now()"
            })
        try:
            supabase.table("live_rankings").upsert(clean_ranks, on_conflict="category,rank").execute()
            print(f"   ğŸ† ë­í‚¹ ê°±ì‹  ì™„ë£Œ")
        except Exception:
            pass

def main():
    print("ğŸš€ ìŠ¤í¬ë˜í¼ ì‹œì‘ (Fixed DDGS Params)")
    
    for category, search_keyword in CATEGORIES.items():
        # 1. ê²€ìƒ‰
        raw_text = search_web(search_keyword)
        
        if len(raw_text) < 10: 
            print(f"âš ï¸ {category} ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€")
            continue

        # 2. AI ìš”ì•½
        data = call_gemini_api(category, raw_text)
        
        # 3. ì €ì¥
        if data:
            update_database(category, data)
        
        time.sleep(3)

    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    main()

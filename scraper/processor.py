# scraper/processor.py
import time
from datetime import datetime
import config
import naver_api
import gemini_api
import database

def run_category_process(category):
    print(f"\nğŸš€ [Processing] Category: {category}")

    # 1. [ê´‘ë²”ìœ„ íƒìƒ‰] APIë¡œ 100ê°œ ìˆ˜ì§‘
    keyword = config.SEARCH_KEYWORDS.get(category)
    raw_items = naver_api.search_news_api(keyword, display=100)
    
    if not raw_items:
        print("   âš ï¸ No items found from Naver API.")
        return

    titles = "\n".join([f"- {item['title']}" for item in raw_items])

    # 2. [ë­í‚¹ ì„ ì •] AIì—ê²Œ Top 10 í‚¤ì›Œë“œ/ìˆœìœ„ ì¶”ì¶œ ìš”ì²­
    # live_rankings í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì— ë§ì¶¤ (rank, title, meta_info, score)
    rank_prompt = f"""
    [Task]
    Analyze these news titles about {category}:
    {titles[:15000]}
    
    1. Identify Top 10 trending keywords (Person, Group, Work).
    2. Provide a short meta info for each (e.g., "New Album", "High Rating").
    
    [Output JSON Format]
    {{
        "rankings": [
            {{ "rank": 1, "keyword": "Name", "meta": "Reason", "score": 95 }}
        ]
    }}
    """
    
    rank_res = gemini_api.ask_gemini(rank_prompt)
    if not rank_res: return

    rankings = rank_res.get("rankings", [])[:10]
    
    # 2-1. ë­í‚¹ DB ì €ì¥ (live_rankings)
    db_rankings = []
    for item in rankings:
        db_rankings.append({
            "category": category,
            "rank": item.get("rank"),
            "title": item.get("keyword"), # DB ì»¬ëŸ¼ëª…ì´ titleì„
            "meta_info": item.get("meta", ""),
            "score": item.get("score", 0),
            "updated_at": datetime.now().isoformat()
        })
    database.save_rankings_to_db(db_rankings)

    # 3. [ì •ë°€ íƒ€ê²©] Top 10 í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜ì§‘ ë° ìš”ì•½
    news_data_list = []
    
    for idx, rank_item in enumerate(rankings):
        keyword = rank_item.get("keyword")
        print(f"   running ({idx+1}/10): {keyword}")
        
        # ê¸°ì‚¬ 2ê°œ ê²€ìƒ‰
        target_items = naver_api.search_news_api(keyword, display=2)
        full_texts = []
        target_link = ""
        target_image = ""

        for item in target_items:
            link = item['link']
            crawled = naver_api.crawl_article(link) # ë³¸ë¬¸+ì´ë¯¸ì§€ ê°€ì ¸ì˜´
            
            if crawled['text']:
                full_texts.append(crawled['text'])
                if not target_image: target_image = crawled['image'] # ì´ë¯¸ì§€ í™•ë³´
                if not target_link: target_link = link
            else:
                full_texts.append(item['description']) # ì‹¤íŒ¨ì‹œ ìš”ì•½ë³¸
                if not target_link: target_link = link

        if not full_texts: continue

        # 4. [ìš”ì•½] AI ê¸°ì‚¬ ì‘ì„±
        # live_news í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì— ë§ì¶¤
        summary_prompt = f"""
        [Articles about '{keyword}']
        {str(full_texts)[:5000]}

        [Task]
        Write a news summary in Korean.
        [Output JSON]
        {{ "title": "Catchy Title", "summary": "3-sentence summary" }}
        """
        
        sum_res = gemini_api.ask_gemini(summary_prompt)
        
        if sum_res:
            news_data_list.append({
                "category": category,
                "keyword": keyword,
                "title": sum_res.get("title", f"{keyword} ì†Œì‹"),
                "summary": sum_res.get("summary", ""),
                "link": target_link,
                "image_url": target_image, # í¬ë¡¤ë§í•œ ì´ë¯¸ì§€
                "score": 100 - idx, # 1ìœ„ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                "created_at": datetime.now().isoformat(),
                "likes": 0
            })
        
        time.sleep(1) # AI ê³¼ë¶€í•˜ ë°©ì§€

    # 5. [ë‰´ìŠ¤ ì €ì¥ & ì²­ì†Œ]
    if news_data_list:
        database.save_news_to_db(news_data_list)
        database.cleanup_old_data(category, "live_news", config.MAX_ITEMS_PER_CATEGORY)

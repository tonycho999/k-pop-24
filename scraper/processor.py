# scraper/processor.py
import time
from datetime import datetime
import config
import naver_api
import gemini_api
import database

def run_category_process(category):
    print(f"\nğŸš€ [Processing] Category: {category}")

    # 1. ìµœì‹  ê¸°ì‚¬ ì œëª© 100ê°œ ìˆ˜ì§‘
    queries = config.SEARCH_QUERIES.get(category, [])
    all_titles = []
    seen_links = set()

    print(f"   1ï¸âƒ£ Collecting 100 latest titles...")
    for q in queries:
        # ê° ì¿¼ë¦¬ë‹¹ 40ê°œì”© ìš”ì²­í•˜ì—¬ ì¤‘ë³µ ì œê±° í›„ 100ê°œ ê·¼ì²˜ë¡œ ë§ì¶¤
        items = naver_api.search_news_api(q, display=40) 
        for item in items:
            if item['link'] not in seen_links:
                seen_links.add(item['link'])
                # HTML íƒœê·¸ ì œê±° ë° ì œëª©ë§Œ ë³´ê´€
                clean_title = item['title'].replace("<b>", "").replace("</b>", "").replace("&quot;", "")
                all_titles.append(clean_title)
        time.sleep(0.3)

    if not all_titles:
        print("   âŒ [Stop] No titles found.")
        return

    print(f"      âœ… Total titles for analysis: {len(all_titles)}")

    # 2. ì œëª© ë¹ˆë„ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (AIì—ê²Œ ì œëª© ë¦¬ìŠ¤íŠ¸ ì „ë‹¬)
    print("   2ï¸âƒ£ AI analyzing frequency & trends...")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì¶œ ê·œì¹™ (ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ 100% ë°˜ì˜)
    if category == "K-Pop":
        rule = "Target: SONG TITLE / Search: ARTIST NAME"
    elif category == "K-Drama":
        rule = "Target: DRAMA TITLE / Search: MAIN ACTOR NAME"
    elif category == "K-Movie":
        rule = "Target: MOVIE TITLE / Search: MAIN ACTOR NAME"
    elif category == "K-Entertain":
        rule = "Target: SHOW TITLE / Search: CAST MEMBER NAME"
    else: # K-Culture
        rule = "Target: PLACE/FOOD/EVENT NAME (English) / Search: Korean Name. EXCLUDE IDOLS."

    rank_prompt = f"""
    [Context]
    Category: {category}
    Below are the latest 100 news titles. Analyze which subjects are mentioned most frequently.

    [Task]
    Identify the Top 10 most mentioned subjects following these rules:
    {rule}

    [News Titles]
    {chr(10).join(all_titles[:100])}

    [Output JSON ONLY]
    {{ "rankings": [ {{ "rank": 1, "display_title_en": "Title", "search_keyword_kr": "SearchName", "meta": "Short reason", "score": 95 }} ] }}
    """
    
    rank_res = gemini_api.ask_gemini(rank_prompt)
    if not rank_res or "rankings" not in rank_res:
        print("   âŒ [Stop] AI failed to extract keywords.")
        return

    rankings = rank_res.get("rankings", [])[:10]
    database.save_rankings_to_db([
        {
            "category": category, "rank": r['rank'], "title": r['display_title_en'],
            "meta_info": r['meta'], "score": r['score'], "updated_at": datetime.now().isoformat()
        } for r in rankings
    ])

    # 3. íƒ€ê²Ÿ ì„ ì • (1ìœ„ í˜¹ì€ ì¿¨íƒ€ì„ ì•„ë‹Œ ê²ƒ)
    target = next((r for r in rankings if not database.is_keyword_used_recently(category, r['display_title_en'])), rankings[0])
    target_display = target['display_title_en']
    target_search = target['search_keyword_kr']
    print(f"   3ï¸âƒ£ Selected: '{target_display}' (Search: {target_search})")

    # 4. ì„ íƒëœ í‚¤ì›Œë“œë¡œ ì •ë°€ ê²€ìƒ‰ (ì´ì œ ì—¬ê¸°ì„œë§Œ ë³¸ë¬¸ì„ ì½ìŒ)
    print(f"   4ï¸âƒ£ Deep dive into '{target_search}'...")
    target_items = naver_api.search_news_api(target_search, display=3)
    
    full_texts = []
    target_link, target_image = "", ""

    for item in target_items:
        crawled = naver_api.crawl_article(item['link'])
        if crawled['text']:
            full_texts.append(crawled['text'])
            if not target_image: target_image = crawled['image']
            if not target_link: target_link = item['link']
        else:
            full_texts.append(item['description'])
            if not target_link: target_link = item['link']

    # 5. ìµœì¢… ì˜ì–´ ìš”ì•½ ì‘ì„±
    print(f"   5ï¸âƒ£ Summarizing news in English...")
    summary_prompt = f"""
    [Topic] {target_display} ({target_search})
    [Articles] {str(full_texts)[:5000]}
    [Task] Write a news summary in ENGLISH.
    [Output JSON] {{ "title": "Headline", "summary": "3-5 sentences" }}
    """
    
    sum_res = gemini_api.ask_gemini(summary_prompt)
    if sum_res:
        news_item = {
            "category": category, "keyword": target_display,
            "title": sum_res.get("title"), "summary": sum_res.get("summary"),
            "link": target_link, "image_url": target_image,
            "score": 100, "created_at": datetime.now().isoformat(), "likes": 0
        }
        database.save_news_to_live([news_item])
        database.save_news_to_archive([news_item])
        database.cleanup_old_data(category, config.MAX_ITEMS_PER_CATEGORY)
        print("   ğŸ‰ SUCCESS!")

import time
from datetime import datetime
import config
import naver_api
import gemini_api
import database

def run_category_process(category):
    print(f"\nğŸš€ [Processing] Category: {category}")

    # ---------------------------------------------------------
    # 1ë‹¨ê³„: 100ê°œ ì´ìƒì˜ ìµœì‹  ë‰´ìŠ¤ ì œëª© ìˆ˜ì§‘ (ê´‘ë²”ìœ„ ê²€ìƒ‰)
    # ---------------------------------------------------------
    all_titles = []
    seen_links = set()
    print(f"   1ï¸âƒ£ Collecting latest news titles for analysis...")
    
    queries = config.SEARCH_QUERIES.get(category, [])
    for q in queries:
        # ìµœì‹ ìˆœ(date)ìœ¼ë¡œ ê° ì¿¼ë¦¬ë‹¹ 50ê°œì”© ê°€ì ¸ì™€ì„œ ì¤‘ë³µ ì œê±°
        items = naver_api.search_news_api(q, display=50, sort='date')
        for item in items:
            if item['link'] not in seen_links:
                seen_links.add(item['link'])
                # ì œëª© ë‚´ HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
                clean_title = item['title'].replace("<b>","").replace("</b>","").replace("&quot;","")
                all_titles.append(clean_title)
        
        if len(all_titles) >= 120: break 
        time.sleep(0.3)

    if not all_titles:
        print(f"   âŒ No titles found for category: {category}")
        return

    # ---------------------------------------------------------
    # 2ë‹¨ê³„: ë­í‚¹ 1~10ìœ„ ì„ ì • ë° ê¸°ì‚¬ ì‘ì„±ìš© íƒ€ê²Ÿ ì¶”ì¶œ
    # ---------------------------------------------------------
    # ì¹´í…Œê³ ë¦¬ë³„ ê·œì¹™ ì„¤ì • (ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ ë°˜ì˜)
    rank_rule = "Target(Rank): SONG / Search(Person): ARTIST" if category == "K-Pop" else \
                "Target(Rank): DRAMA / Search(Person): ACTOR" if category == "K-Drama" else \
                "Target(Rank): MOVIE / Search(Person): ACTOR" if category == "K-Movie" else \
                "Target(Rank): SHOW / Search(Person): CAST" if category == "K-Entertain" else \
                "Target: PLACE or TRADITION / Search: KEYWORD (EXCLUDE IDOLS)"

    print(f"   2ï¸âƒ£ AI analyzing trends from {len(all_titles[:100])} titles...")
    rank_prompt = f"""
    Analyze these news titles about {category}. 
    
    [Task]
    1. Identify the TOP 10 {rank_rule.split('/')[0]} mentioned most frequently in these titles.
    2. Pick the SINGLE most trending {rank_rule.split('/')[1]} to be the subject of a deep-dive article.
    
    [Titles Data]
    {" | ".join(all_titles[:100])}
    
    [Important Rules]
    - 'search_keyword_kr' MUST be in KOREAN (e.g., 'ë‰´ì§„ìŠ¤', 'ì´ì •ì¬', 'ê²½ë³µê¶').
    - 'display_title_en' and 'top_subject_en' MUST be in ENGLISH.
    - For K-Culture: Strictly exclude K-Pop idols or celebrities.
    
    [Return JSON Format]
    {{
      "rankings": [ 
        {{
          "rank": 1, 
          "display_title_en": "English Title", 
          "search_keyword_kr": "í•œêµ­ì–´ ê²€ìƒ‰ì–´", 
          "meta": "Brief trending reason in English", 
          "score": 95
        }} 
      ],
      "top_person_kr": "í•œêµ­ì–´ ê²€ìƒ‰ì–´(ê°€ìˆ˜/ë°°ìš°/ì¥ì†Œëª…)",
      "top_subject_en": "English Subject Name for Database"
    }}
    """
    
    rank_res = gemini_api.ask_gemini(rank_prompt)
    if not rank_res or "rankings" not in rank_res:
        print("   âŒ AI failed to extract ranking data.")
        return

    # ë¼ì´ë¸Œ ë­í‚¹ DB ì—…ë°ì´íŠ¸
    database.save_rankings_to_db(rank_res.get("rankings", []))
    
    # ---------------------------------------------------------
    # 5ë‹¨ê³„ ì ìš©: ìµœê·¼ 4ì‹œê°„ ë‚´ ì‚¬ìš©ëœ í‚¤ì›Œë“œì¸ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
    # ---------------------------------------------------------
    target_kr = rank_res.get("top_person_kr") # ë„¤ì´ë²„ ì¬ê²€ìƒ‰ìš© (í•œêµ­ì–´)
    target_en = rank_res.get("top_subject_en") # DB ì €ì¥ìš© (ì˜ì–´)

    if database.is_keyword_used_recently(category, target_en, hours=4):
        print(f"   ğŸ•’ '{target_en}' is on 4-hour cooldown. Skipping article generation.")
        return

    # ---------------------------------------------------------
    # 3ë‹¨ê³„: ì„ íƒëœ í‚¤ì›Œë“œë¡œ ì •ë°€ ê²€ìƒ‰ ë° ë³¸ë¬¸ 3ê°œ ìƒ˜í”Œë§
    # ---------------------------------------------------------
    print(f"   3ï¸âƒ£ Deep searching for '{target_kr}' (Sampling 3 valid articles)...")
    deep_items = naver_api.search_news_api(target_kr, display=10, sort='date')
    
    full_texts = []
    main_link = ""
    main_image = ""
    
    for item in deep_items:
        crawled = naver_api.crawl_article(item['link'])
        # ë³¸ë¬¸ì´ ì¶©ë¶„íˆ ê¸¸ê³  ìœ íš¨í•œ ê²½ìš°ë§Œ ìˆ˜ì§‘
        if crawled['text'] and len(crawled['text']) > 300:
            full_texts.append(crawled['text'])
            if not main_link: main_link = item['link']
            if not main_image: main_image = crawled['image']
        
        # 3ê°œì˜ ì„±ê³µì ì¸ ë³¸ë¬¸ì„ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
        if len(full_texts) >= 3:
            break

    if len(full_texts) < 1:
        print(f"   âŒ Could not retrieve enough article bodies for '{target_kr}'.")
        return

    # ---------------------------------------------------------
    # 4ë‹¨ê³„: ë² í…Œë‘ ê¸°ì ìŠ¤íƒ€ì¼ë¡œ ìƒˆë¡œìš´ ì˜ì–´ ê¸°ì‚¬ ì‘ì„±
    # ---------------------------------------------------------
    print(f"   4ï¸âƒ£ Writing Professional Article in English (20-year Veteran Style)...")
    article_prompt = f"""
    You are a veteran entertainment journalist with 20 years of experience. 
    Write a NEW, insightful professional news report in ENGLISH based on the provided 3 Korean articles.

    [Subject]
    {target_en} ({target_kr})

    [Source Material (Korean)]
    {str(full_texts)[:6000]}

    [Requirements]
    - Headline: Catchy, authoritative, and professional.
    - Content: Write 4-5 paragraphs of in-depth analysis. 
    - Style: Do NOT just summarize. Create a new narrative that connects the facts with expert insight.
    - Language: Perfect journalistic English.

    [Output JSON Format]
    {{ "title": "Headline", "content": "Full Professional Article Body" }}
    """
    
    news_res = gemini_api.ask_gemini(article_prompt)
    
    if news_res and news_res.get("content"):
        news_item = {
            "category": category,
            "keyword": target_en,
            "title": news_res.get("title"),
            "summary": news_res.get("content"), # ì „ë¬¸ ë‚´ìš©ì„ summary í•„ë“œì— ì €ì¥
            "link": main_link,
            "image_url": main_image,
            "score": 100,
            "created_at": datetime.now().isoformat(),
            "likes": 0
        }
        
        # ìµœì¢… DB ì €ì¥
        database.save_news_to_live([news_item])
        database.save_news_to_archive([news_item])
        database.cleanup_old_data(category, config.MAX_ITEMS_PER_CATEGORY)
        print(f"   ğŸ‰ SUCCESS: '{target_en}' article has been published.")
    else:
        print("   âŒ AI failed to generate the final article.")

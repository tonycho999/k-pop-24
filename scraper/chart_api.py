import asyncio
import json  # <--- ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤!
import os
from playwright.async_api import async_playwright

class ChartEngine:
    def __init__(self):
        pass

    def get_top10_chart(self, category):
        """
        ë´‡ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        if category == "k-pop":
            print(f"ðŸ” [Bot] Scraping Melon Real-time Chart...")
            try:
                # Playwright ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ì²˜ë¦¬
                return asyncio.run(self._scrape_melon())
            except Exception as e:
                print(f"âŒ Scraping Error: {e}")
                return json.dumps({"top10": []})
        else:
            return json.dumps({"top10": []})

    async def _scrape_melon(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                # ë©œë¡  ì°¨íŠ¸ ì ‘ì†
                await page.goto("https://www.melon.com/chart/index.htm", timeout=60000)
                await page.wait_for_selector(".lst50", timeout=10000)

                top10_data = []
                rows = await page.query_selector_all(".lst50")
                
                for i, row in enumerate(rows[:10]):
                    # ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê³¡ëª…, ê°€ìˆ˜ëª…)
                    title_el = await row.query_selector(".rank01 a")
                    artist_el = await row.query_selector(".rank02 a")
                    
                    title = (await title_el.inner_text()).strip()
                    artist = (await artist_el.inner_text()).strip()
                    
                    top10_data.append({
                        "rank": i + 1,
                        "title": title,
                        "info": artist
                    })

                await browser.close()
                # ensure_ascii=Falseë¥¼ í•´ì¤˜ì•¼ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê³  JSONìœ¼ë¡œ ì €ìž¥ë©ë‹ˆë‹¤.
                return json.dumps({"top10": top10_data}, ensure_ascii=False)
            
            except Exception as e:
                print(f"âŒ Bot Scraping Error: {e}")
                await browser.close()
                return json.dumps({"top10": []})

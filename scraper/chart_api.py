import os
import json
import requests
import time
import random
import email.utils
from datetime import datetime, timedelta
from groq import Groq

class ChartEngine:
    def __init__(self):
        self.groq_client = None
        self.kobis_key = os.environ.get("KOBIS_API_KEY")
        self.selected_model = None

    def set_groq_client(self, api_key):
        """API í‚¤ ì„¤ì • ë° ì‹¤ì‹œê°„ ê°€ìš© ëª¨ë¸ ìë™ ì„ íƒ"""
        self.groq_client = Groq(api_key=api_key)
        self._auto_select_model()

    def _auto_select_model(self):
        """Groq ê°€ìš© ëª¨ë¸ ì¤‘ ìµœì ì˜ ëª¨ë¸ ì„ íƒ"""
        try:
            models = self.groq_client.models.list()
            model_ids = [m.id for m in models.data]
            preferences = [
                "llama-3.3-70b-specdec",
                "llama-3.1-70b-versatile",
                "llama-3.1-8b-instant"
            ]
            for pref in preferences:
                if pref in model_ids:
                    self.selected_model = pref
                    print(f"ğŸ¤– AI Model Selected: {self.selected_model}")
                    return
            self.selected_model = model_ids[0]
        except Exception as e:
            print(f"âŒ Model selection error: {e}")
            self.selected_model = "llama-3.1-8b-instant"

    def get_top10_chart(self, category):
        """24ì‹œê°„ ì´ë‚´ì˜ ìµœì‹  ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ì—¬ ì˜ë¬¸ìœ¼ë¡œ ë²ˆì—­ ë°˜í™˜"""
        max_retries = 1
        for attempt in range(max_retries + 1):
            try:
                # API í˜¸ì¶œ ê°„ê²© ìœ ì§€ (ëœë¤ ëŒ€ê¸°)
                wait_time = random.uniform(3.0, 5.0)
                time.sleep(wait_time)

                if category == "k-movie":
                    # ì˜í™”ëŠ” ê³µì‹ APIì—ì„œ ì–´ì œ(24ì‹œê°„ ë‚´) ë°ì´í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì˜´
                    raw_data = self._get_kobis_movie()
                else:
                    # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ì–´ (ìµœì‹ ì„± ìœ ë„ë¥¼ ìœ„í•´ 'ì˜¤ëŠ˜', 'ì‹¤ì‹œê°„' ê°•ì¡°)
                    queries = {
                        "k-pop": "ì˜¤ëŠ˜ ì‹¤ì‹œê°„ ìŒì› ì°¨íŠ¸ ìˆœìœ„ ë©œë¡  ì¨í´ì°¨íŠ¸",
                        "k-drama": "ì˜¤ëŠ˜ ë“œë¼ë§ˆ ì‹œì²­ë¥  ìˆœìœ„ ë‹ìŠ¨ì½”ë¦¬ì•„",
                        "k-entertain": "ì˜¤ëŠ˜ ì˜ˆëŠ¥ ì‹œì²­ë¥  ìˆœìœ„ ë‹ìŠ¨ì½”ë¦¬ì•„",
                        "k-culture": "ì˜¤ëŠ˜ ì„±ìˆ˜ë™ í•œë‚¨ë™ íŒì—…ìŠ¤í† ì–´ í•«í”Œë ˆì´ìŠ¤ ì¶”ì²œ"
                    }
                    raw_data = self._get_fresh_news_data(category, queries.get(category))

                # ë¶„ì„ ë° ì˜ë¬¸ ë²ˆì—­
                return self._ai_extract_and_translate(category, raw_data)

            except Exception as e:
                if attempt < max_retries:
                    print(f"âš ï¸ [{category}] Retry (Attempt {attempt+2}): {e}")
                    time.sleep(5)
                else:
                    print(f"âŒ [{category}] Skipped: {e}")
                    return json.dumps({"top10": []})

    def _get_fresh_news_data(self, category, query):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ì •í™•íˆ 24ì‹œê°„ ì´ë‚´ì˜ ê¸°ì‚¬ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ì¶œ"""
        client_id = os.environ.get("NAVER_CLIENT_ID")
        client_secret = os.environ.get("NAVER_CLIENT_SECRET")
        
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=30&sort=date"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        items = res.json().get('items', [])
        
        now = datetime.now()
        fresh_contents = []
        
        for item in items:
            # ë„¤ì´ë²„ ë‚ ì§œ í˜•ì‹(RFC822) íŒŒì‹±
            pub_date = email.utils.parsedate_to_datetime(item['pubDate']).replace(tzinfo=None)
            
            # ì •í™•íˆ í˜„ì¬ ì‹œê°„ìœ¼ë¡œë¶€í„° 24ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ í†µê³¼
            if now - pub_date <= timedelta(hours=24):
                fresh_contents.append(f"[{pub_date.strftime('%H:%M')}] {item['title']} {item['description']}")

        if not fresh_contents:
            raise ValueError(f"No fresh news found for {category} within the last 24 hours.")
            
        print(f"âœ… Found {len(fresh_contents)} fresh news items for {category}.")
        return "\n".join(fresh_contents)[:5000]

    def _get_kobis_movie(self):
        """ì˜í™”ì§„í¥ìœ„ì›íšŒ API (ì–´ì œ ë‚ ì§œ ê³ ì •)"""
        target_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        url = f"http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json?key={self.kobis_key}&targetDt={target_date}"
        res = requests.get(url, timeout=10)
        return res.text

    def _ai_extract_and_translate(self, category, raw_data):
        """AIë¥¼ í†µí•œ ë°ì´í„° ë¶„ì„ ë° ì˜ë¬¸ ë²ˆì—­"""
        prompt = f"""
        Analyze the provided South Korean news snippets from the LAST 24 HOURS to extract the {category} Top 10.
        
        [STRICT GUIDELINES]
        1. TIME SENSITIVITY: Use ONLY data from the provided text. Ensure it represents current trends.
        2. TRANSLATION: Translate the 'title' and 'info' into English.
        3. PROPER NOUNS: Use official English names for artists and shows (e.g., 'NewJeans' instead of 'Nyujinseu', 'IU' instead of 'Aiyu').
        4. ACCURACY: If the text doesn't provide a clear ranking, list the most discussed topics/items in the text.
        5. OUTPUT: Respond ONLY with a JSON object in this format:
           {{"top10": [{{"rank": 1, "title": "English Title", "info": "Brief English Info"}}, ...]}}
        
        Data (Last 24h):
        {raw_data}
        """
        
        chat = self.groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=self.selected_model,
            response_format={"type": "json_object"},
            temperature=0.1
        )
        return chat.choices[0].message.content

import asyncio
import json
from playwright.async_api import async_playwright

class ChartEngine:
    def __init__(self):
        self.ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        self.rotation_map = {
            "k-pop": ["melon", "genie", "bugs"],
            "k-drama": ["naver_search", "naver_search", "naver_search"],
            "k-movie": ["naver_search", "naver_search", "naver_search"],
            "k-entertain": ["naver_search", "naver_search", "naver_search"]
        }

    async def get_top10_chart(self, category, run_count):
        targets = self.rotation_map.get(category, ["naver_search"])
        target = targets[run_count % 3]
        
        print(f"ğŸ” [Attempt] Category: {category} | Primary: {target}")
        result = await self._scrape_entry(target, category)
        
        # ë©”ì¸ íƒ€ê²Ÿ ì‹¤íŒ¨ ì‹œ ë„¤ì´ë²„ ë°±ì—… ì‹¤í–‰
        if not result or len(result) < 3:
            print(f"âš ï¸ {target} failed/insufficient. Switching to Emergency Backup: naver_search")
            result = await self._scrape_entry("naver_search", category)
            
        return json.dumps({"top10": result}, ensure_ascii=False)

    async def _scrape_entry(self, target, category):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            # ê¶Œí•œ ë° ì–¸ì–´ ì„¤ì • ì¶”ê°€ (ì°¨ë‹¨ ë°©ì§€)
            context = await browser.new_context(user_agent=self.ua, locale="ko-KR")
            page = await context.new_page()
            data = []
            
            try:
                if target == "melon":
                    await page.goto("https://www.melon.com/chart/index.htm", timeout=30000)
                    await page.wait_for_selector(".lst50", timeout=10000)
                    rows = await page.query_selector_all(".lst50")
                    for i, r in enumerate(rows[:10]):
                        t = await (await r.query_selector(".rank01 a")).inner_text()
                        a = await (await r.query_selector(".rank02 a")).inner_text()
                        data.append({"rank": i+1, "title": t.strip(), "info": a.strip()})
                
                elif target == "naver_search":
                    # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ì–´ ìµœì í™”
                    queries = {
                        "k-pop": "ë©œë¡  ì°¨íŠ¸ ìˆœìœ„",
                        "k-drama": "ë“œë¼ë§ˆ ì‹œì²­ë¥  ìˆœìœ„",
                        "k-movie": "ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ìˆœìœ„",
                        "k-entertain": "ì˜ˆëŠ¥ ì‹œì²­ë¥  ìˆœìœ„"
                    }
                    search_url = f"https://search.naver.com/search.naver?query={queries.get(category, category)}"
                    await page.goto(search_url, timeout=30000)
                    
                    # ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸° (ì¤‘ìš”)
                    await page.wait_for_load_state("networkidle")
                    await page.mouse.wheel(0, 500) # ì•½ê°„ì˜ ìŠ¤í¬ë¡¤ë¡œ ë¡œë”© ìœ ë„
                    await asyncio.sleep(2) # ì•ˆì •ì ì¸ ë¡œë”©ì„ ìœ„í•œ ëŒ€ê¸°

                    # [ìˆ˜ì •ëœ Selector] ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ (2026 ê¸°ì¤€ ëŒ€ì‘)
                    # ì‹œì²­ë¥ /ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ê³µí†µ ìš”ì†Œë¥¼ ë” ë„“ê²Œ ì¡ìŒ
                    items = await page.query_selector_all(".api_subject_bx .list_box .item, .api_subject_bx .lst_common .item")
                    
                    if not items:
                        # ëŒ€ì•ˆ Selector ì‹œë„ (ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ì „ìš© ë“±)
                        items = await page.query_selector_all(".box_image_list .item, .movie_audience_ranking .item")

                    for i, item in enumerate(items[:10]):
                        # ì œëª© ì°¾ê¸°
                        title_el = await item.query_selector(".name, .title, .tit")
                        # ì •ë³´(ì‹œì²­ë¥ /ê´€ê°ìˆ˜) ì°¾ê¸°
                        info_el = await item.query_selector(".figure, .sub_text, .value")
                        
                        if title_el:
                            t_text = await title_el.inner_text()
                            i_text = await info_el.inner_text() if info_el else ""
                            data.append({"rank": i+1, "title": t_text.strip(), "info": i_text.strip()})

                await browser.close()
                return data

            except Exception as e:
                print(f"âŒ Scrape Error ({target} - {category}): {e}")
                # ì‹¤íŒ¨ ì‹œ ë¡œê·¸ìš© ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                await page.screenshot(path=f"debug_{category}.png")
                await browser.close()
                return None

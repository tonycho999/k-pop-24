import asyncio
import json
import os
from playwright.async_api import async_playwright

class ChartEngine:
    def __init__(self):
        self.ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        # ì¹´í…Œê³ ë¦¬ë³„ 3ì‚¬ ë¡œí…Œì´ì…˜ ë§µ
        self.rotation_map = {
            "k-pop": ["melon", "genie", "bugs"],
            "k-drama": ["nielsen", "naver_drama", "daum_drama"],
            "k-movie": ["kobis", "naver_movie", "daum_movie"],
            "k-entertain": ["nielsen_ent", "naver_ent", "daum_ent"]
        }

    async def get_top10_chart(self, category, run_count):
        targets = self.rotation_map.get(category, ["naver_search"])
        target = targets[run_count % 3] # 0, 1, 2 ìˆœí™˜
        
        print(f"ğŸ” [Attempt] Category: {category} | Primary: {target}")
        
        # 1. ë©”ì¸ íƒ€ê²Ÿ ì‹œë„
        result = await self._scrape_entry(target, category)
        
        # 2. ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ë„¤ì´ë²„ í†µí•© ê²€ìƒ‰(ë°±ì—…) ì‹œë„
        if not result or len(result) < 5:
            print(f"âš ï¸ {target} failed or insufficient. Switching to Backup: naver_search")
            result = await self._scrape_entry("naver_search", category)
            
        return json.dumps({"top10": result}, ensure_ascii=False)

    async def _scrape_entry(self, target, category):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(user_agent=self.ua)
            data = []
            try:
                # [ì˜ˆì‹œ] íƒ€ê²Ÿë³„ ë¶„ê¸° ì²˜ë¦¬ (ì‹¤ì œ ì‚¬ì´íŠ¸ë³„ Selector ì ìš© í•„ìš”)
                if target == "melon":
                    await page.goto("https://www.melon.com/chart/index.htm", timeout=30000)
                    rows = await page.query_selector_all(".lst50")
                    for i, r in enumerate(rows[:10]):
                        t = await (await r.query_selector(".rank01 a")).inner_text()
                        a = await (await r.query_selector(".rank02 a")).inner_text()
                        data.append({"rank": i+1, "title": t.strip(), "info": a.strip()})
                
                elif target == "naver_search":
                    queries = {"k-pop":"ë©œë¡ ì°¨íŠ¸", "k-drama":"ë“œë¼ë§ˆ ì‹œì²­ë¥ ", "k-movie":"ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "k-entertain":"ì˜ˆëŠ¥ ì‹œì²­ë¥ "}
                    await page.goto(f"https://search.naver.com/search.naver?query={queries.get(category, category)}")
                    await page.wait_for_timeout(2000)
                    items = await page.query_selector_all(".api_subject_bx .list_box .item")
                    for i, item in enumerate(items[:10]):
                        title_el = await item.query_selector(".name, .title")
                        if title_el:
                            data.append({"rank": i+1, "title": (await title_el.inner_text()).strip(), "info": ""})

                # (ì§€ë‹ˆ, ë²…ìŠ¤, ë‹ìŠ¨ ë“± ì¶”ê°€ íƒ€ê²Ÿ ë¡œì§ êµ¬í˜„...)
                
                await browser.close()
                return data
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ê°€ ë‚œ ì‹œì ì˜ HTMLì„ ì €ì¥í•˜ì—¬ AI ë¶„ì„ìš©ìœ¼ë¡œ ë„˜ê¹€
                print(f"âŒ Scrape Error ({target}): {e}")
                html_content = await page.content()
                with open(f"error_{category}.html", "w", encoding="utf-8") as f:
                    f.write(html_content)
                await browser.close()
                return None

import os
import json
import urllib.parse
import urllib.request
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
import feedparser 

def get_naver_api_news(keyword):
    """ë„¤ì´ë²„ API ë‰´ìŠ¤ ê²€ìƒ‰ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)"""
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=date"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    
    try:
        # print(f"ğŸ“¡ ë„¤ì´ë²„ API í˜¸ì¶œ ì¤‘: {keyword}...")
        res = urllib.request.urlopen(req, timeout=10) 
        items = json.loads(res.read().decode('utf-8')).get('items', [])
        
        valid_items = []
        now = datetime.now()
        threshold = now - timedelta(hours=24)

        for item in items:
            try:
                pub_date = parsedate_to_datetime(item['pubDate']).replace(tzinfo=None)
                if pub_date < threshold:
                    continue
                item['published_at'] = pub_date
                valid_items.append(item)
            except:
                continue

        return valid_items

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ API ì—ëŸ¬ ({keyword}): {e}")
        return []

def get_article_data(link):
    """
    [ì—…ê·¸ë ˆì´ë“œ] ê¸°ì‚¬ ë³¸ë¬¸(1,500ì) ë° ì´ë¯¸ì§€ í†µí•© ì¶”ì¶œ í•¨ìˆ˜
    * ìˆ˜ì •ì‚¬í•­: Mixed Content ë°©ì§€ë¥¼ ìœ„í•´ HTTPS ì´ë¯¸ì§€ ê°•ì œ
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        # íƒ€ì„ì•„ì›ƒ 5ì´ˆ ì„¤ì •
        res = requests.get(link, headers=headers, timeout=5)
        
        if res.status_code != 200:
            return "", None

        soup = BeautifulSoup(res.text, 'html.parser')
        
        # --- 1. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        content_area = soup.select_one('#dic_area, #articleBodyContents, .article_view, #articeBody, .news_view, #newsct_article, .article-body')
        
        full_text = ""
        if content_area:
            for s in content_area(['script', 'style', 'iframe', 'button', 'a', 'div.ad']):
                s.decompose()
            full_text = content_area.get_text(separator=' ', strip=True)
            full_text = full_text[:1500]
        else:
            full_text = soup.body.get_text(separator=' ', strip=True)[:1000] if soup.body else ""

        # --- 2. ì´ë¯¸ì§€ ì¶”ì¶œ (HTTPS ê°•ì œ) ---
        image_url = None
        
        if content_area:
            imgs = content_area.find_all('img')
            for i in imgs:
                src = i.get('src') or i.get('data-src')
                # http:// ëŠ” ë²„ë¦¬ê³  ë°˜ë“œì‹œ https:// ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë§Œ ê°€ì ¸ì˜´
                if src and src.startswith('https://'):
                    width = i.get('width')
                    if width and width.isdigit() and int(width) < 200: continue
                    image_url = src
                    break

        if not image_url:
            og = soup.find('meta', property='og:image')
            if og and og.get('content'): 
                candidate = og['content']
                if candidate.startswith('https://'):
                    image_url = candidate

        if image_url:
            bad_keywords = r'logo|icon|button|share|banner|thumb|profile|default|ranking|news_stand|ssl.pstatic.net'
            if re.search(bad_keywords, image_url, re.IGNORECASE): 
                image_url = None

        return full_text, image_url

    except Exception as e:
        return "", None

def get_google_trending_keywords():
    """
    [ìˆ˜ì •] êµ¬ê¸€ íŠ¸ë Œë“œ RSS ìˆ˜ì§‘ (ì°¨ë‹¨ ìš°íšŒ ì ìš©)
    - feedparserë¡œ ë°”ë¡œ í˜¸ì¶œí•˜ì§€ ì•Šê³ , requestsë¡œ User-Agent í—¤ë”ë¥¼ ë‹¬ì•„ì„œ í˜¸ì¶œ
    """
    try:
        url = "https://trends.google.co.kr/trends/trendingsearches/daily/rss?geo=KR"
        
        # ë´‡ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € í—¤ë” ìœ„ì¥
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # 1. requestsë¡œ ë°ì´í„° ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            # 2. ê°€ì ¸ì˜¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ feedparserì— ì „ë‹¬
            feed = feedparser.parse(response.text)
            keywords = [entry.title for entry in feed.entries]
            return keywords
        else:
            print(f"âš ï¸ êµ¬ê¸€ íŠ¸ë Œë“œ ì‘ë‹µ ì½”ë“œ ì—ëŸ¬: {response.status_code}")
            return []
            
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

import os
import sys
import urllib.request
import urllib.parse
import json
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

def get_naver_api_news(keyword, display=10, sort='sim'):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í˜¸ì¶œ
    """
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    try:
        encText = urllib.parse.quote(keyword)
        url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display}&start=1&sort={sort}"

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
        request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)

        response = urllib.request.urlopen(request)
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            items = []
            for item in data.get('items', []):
                clean_item = {
                    'title': BeautifulSoup(item['title'], 'html.parser').get_text(),
                    'link': item['link'],
                    'description': BeautifulSoup(item['description'], 'html.parser').get_text(),
                    'pubDate': item['pubDate']
                }
                items.append(clean_item)
            return items
        return []
    except Exception as e:
        print(f"âš ï¸ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

def get_article_data(url, target_keyword=None):
    """
    [Updated] ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ë° í‚¤ì›Œë“œ ê²€ì¦
    target_keywordê°€ ë³¸ë¬¸ì— ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•˜ì—¬ ìˆ˜ì§‘ ëŒ€ìƒì—ì„œ ì œì™¸í•¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        # íƒ€ì„ì•„ì›ƒ 3ì´ˆ (ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´)
        response = requests.get(url, headers=headers, timeout=3)
        if response.status_code != 200: return None, None
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ì´ë¯¸ì§€ ì¶”ì¶œ
        image_url = None
        og_image = soup.find("meta", property="og:image")
        if og_image: image_url = og_image.get("content")
        
        # 2. ë³¸ë¬¸ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ vs ì¼ë°˜)
        content = ""
        if "news.naver.com" in url:
            article_body = soup.find('div', id='dic_area') or soup.find('div', id='articleBodyContents')
            if article_body: content = article_body.get_text(strip=True)
        else:
            paragraphs = soup.find_all('p')
            content = " ".join([p.get_text(strip=True) for p in paragraphs])

        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(ê´‘ê³  ë“±) ë¬´ì‹œ
        if len(content) < 100: 
            return None, None

        # ğŸš¨ [ê²€ì¦ ë¡œì§] ë³¸ë¬¸ì— íƒ€ê²Ÿ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if target_keyword:
            # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì²´í¬
            if target_keyword.lower() not in content.lower():
                # print(f"      ğŸ—‘ï¸ [Skip] ë³¸ë¬¸ì— '{target_keyword}' ì—†ìŒ.") # ë””ë²„ê¹…ìš©
                return None, None

        return content[:1800], image_url 

    except Exception:
        return None, None

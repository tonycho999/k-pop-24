# scraper/database.py
import os
from datetime import datetime, timedelta
from supabase import create_client, Client
from dotenv import load_dotenv

# ìƒìœ„ í´ë”ì˜ .env ë¡œë“œ
load_dotenv(os.path.join(os.path.dirname(__file__), '../.env'))

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

supabase: Client = None
try:
    if SUPABASE_URL and SUPABASE_KEY:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    else:
        print("ğŸš¨ Supabase credentials missing in .env")
except Exception as e:
    print(f"ğŸš¨ Supabase Connection Error: {e}")

def is_keyword_used_recently(category, keyword, hours=4):
    """
    [ë„ë°° ë°©ì§€] í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œê°€ ìµœê·¼ Nì‹œê°„ ë‚´ì— ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
    True = ì´ë¯¸ ì”€ (ì‚¬ìš© ë¶ˆê°€) / False = ì•ˆ ì”€ (ì‚¬ìš© ê°€ëŠ¥)
    """
    if not supabase: return False
    
    try:
        # í˜„ì¬ ì‹œê°„(UTC) - Nì‹œê°„
        time_limit = (datetime.utcnow() - timedelta(hours=hours)).isoformat()
        
        # live_news í…Œì´ë¸”ì—ì„œ ê²€ì‚¬
        res = supabase.table("live_news")\
            .select("id", count="exact")\
            .eq("category", category)\
            .eq("keyword", keyword)\
            .gte("created_at", time_limit)\
            .execute()
            
        # ì¹´ìš´íŠ¸ê°€ 0ë³´ë‹¤ í¬ë©´ ì´ë¯¸ ì“´ ê²ƒ
        return res.count > 0
    except Exception as e:
        print(f"   âš ï¸ DB Check Error: {e}")
        return False

def save_news_to_live(data_list):
    """[ë©”ì¸ ì „ì‹œìš©] live_news í…Œì´ë¸”ì— ì €ì¥ (ìµœì‹  30ê°œ ìœ ì§€ìš©)"""
    if not supabase or not data_list: return

    try:
        supabase.table("live_news").upsert(data_list).execute()
        print(f"   ğŸ’¾ [Live] Saved to 'live_news'.")
    except Exception as e:
        print(f"   âš ï¸ DB Save Error (live_news): {e}")

def save_news_to_archive(data_list):
    """[ì˜êµ¬ ë³´ê´€ìš©] search_archive í…Œì´ë¸”ì— ì €ì¥ (ì‚­ì œ ì•ˆ í•¨)"""
    if not supabase or not data_list: return

    try:
        # ì•„ì¹´ì´ë¸ŒëŠ” upsert ëŒ€ì‹  insert (íˆìŠ¤í† ë¦¬ ë³´ì¡´)
        supabase.table("search_archive").insert(data_list).execute()
        print(f"   ğŸ“¦ [Archive] Saved to 'search_archive'.")
    except Exception as e:
        print(f"   âš ï¸ DB Save Error (search_archive): {e}")

def save_rankings_to_db(rank_list):
    """[ìˆœìœ„í‘œ] live_rankings í…Œì´ë¸”ì— ì €ì¥"""
    if not supabase or not rank_list: return

    try:
        supabase.table("live_rankings").upsert(rank_list).execute()
        print(f"   ğŸ† Saved rankings.")
    except Exception as e:
        print(f"   âš ï¸ DB Save Error (live_rankings): {e}")

def cleanup_old_data(category, max_limit=30):
    """[ì²­ì†Œ] live_news í…Œì´ë¸”ì—ì„œ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ (30ê°œ ìœ ì§€)"""
    if not supabase: return

    try:
        # 1. ê°œìˆ˜ í™•ì¸
        res = supabase.table("live_news").select("id", count="exact").eq("category", category).execute()
        count = res.count

        if count > max_limit:
            # 2. ì§€ì›Œì•¼ í•  ê°œìˆ˜ ê³„ì‚°
            items_to_remove = count - max_limit
            
            # 3. ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ID ì¡°íšŒ
            old_rows = supabase.table("live_news")\
                .select("id")\
                .eq("category", category)\
                .order("created_at", desc=False)\
                .limit(items_to_remove)\
                .execute()
            
            ids = [row['id'] for row in old_rows.data]
            
            if ids:
                supabase.table("live_news").delete().in_("id", ids).execute()
                print(f"   ğŸ§¹ [Cleanup] Removed {len(ids)} old items from 'live_news'.")
                
    except Exception as e:
        print(f"   âš ï¸ Cleanup Error: {e}")
